{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle San Francisco Crime Classification\n",
    "## Berkeley MIDS W207 Final Project: Sam Goodgame, Sarah Cha, Kalvin Kao, Bryan Moore\n",
    "### Basic Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalvi\\AppData\\Local\\conda\\conda\\envs\\py2All1\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\kalvi\\AppData\\Local\\conda\\conda\\envs\\py2All1\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries:\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import Meta-estimators\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Import Calibration tools\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Set random seed and format print output:\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDL to construct table for SQL transformations:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE kaggle_sf_crime (\n",
    "dates TIMESTAMP,                                \n",
    "category VARCHAR,\n",
    "descript VARCHAR,\n",
    "dayofweek VARCHAR,\n",
    "pd_district VARCHAR,\n",
    "resolution VARCHAR,\n",
    "addr VARCHAR,\n",
    "X FLOAT,\n",
    "Y FLOAT);\n",
    "```\n",
    "#### Getting training data into a locally hosted PostgreSQL database:\n",
    "```sql\n",
    "\\copy kaggle_sf_crime FROM '/Users/Goodgame/Desktop/MIDS/207/final/sf_crime_train.csv' DELIMITER ',' CSV HEADER;\n",
    "```\n",
    "\n",
    "#### SQL Query used for transformations:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  category,\n",
    "  date_part('hour', dates) AS hour_of_day,\n",
    "  CASE\n",
    "    WHEN dayofweek = 'Monday' then 1\n",
    "    WHEN dayofweek = 'Tuesday' THEN 2\n",
    "    WHEN dayofweek = 'Wednesday' THEN 3\n",
    "    WHEN dayofweek = 'Thursday' THEN 4\n",
    "    WHEN dayofweek = 'Friday' THEN 5\n",
    "    WHEN dayofweek = 'Saturday' THEN 6\n",
    "    WHEN dayofweek = 'Sunday' THEN 7\n",
    "  END AS dayofweek_numeric,\n",
    "  X,\n",
    "  Y,\n",
    "  CASE\n",
    "    WHEN pd_district = 'BAYVIEW' THEN 1\n",
    "    ELSE 0\n",
    "  END AS bayview_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'INGLESIDE' THEN 1\n",
    "    ELSE 0\n",
    "  END AS ingleside_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'NORTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS northern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'CENTRAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS central_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'BAYVIEW' THEN 1\n",
    "    ELSE 0\n",
    "  END AS pd_bayview_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'MISSION' THEN 1\n",
    "    ELSE 0\n",
    "  END AS mission_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'SOUTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS southern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TENDERLOIN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS tenderloin_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'PARK' THEN 1\n",
    "    ELSE 0\n",
    "  END AS park_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'RICHMOND' THEN 1\n",
    "    ELSE 0\n",
    "  END AS richmond_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TARAVAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS taraval_binary\n",
    "FROM kaggle_sf_crime;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data into training, development, and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['KIDNAPPING', 'WEAPON LAWS', 'SECONDARY CODES', 'WARRANTS', 'PROSTITUTION', 'EMBEZZLEMENT', 'LOITERING', 'SUICIDE', 'DRIVING UNDER THE INFLUENCE', 'VEHICLE THEFT', 'ROBBERY', 'BURGLARY', 'SUSPICIOUS OCC', 'ARSON', 'BRIBERY', 'FORGERY/COUNTERFEITING', 'BAD CHECKS', 'DRUNKENNESS', 'GAMBLING', 'OTHER OFFENSES', 'FRAUD', 'RECOVERED VEHICLE', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'MISSING PERSON', 'EXTORTION', 'LIQUOR LAWS', 'SEX OFFENSES NON FORCIBLE', 'SEX OFFENSES FORCIBLE', 'STOLEN PROPERTY', 'ASSAULT', 'FAMILY OFFENSES', 'NON-CRIMINAL', 'DISORDERLY CONDUCT', 'RUNAWAY'])\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/train_transformed.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "x_data = df.drop('category', 1)\n",
    "y = df.category.as_matrix()\n",
    "\n",
    "# Impute missing values with mean values:\n",
    "x_complete = x_data.fillna(x_data.mean())\n",
    "X_raw = x_complete.as_matrix()\n",
    "\n",
    "# Scale the data between 0 and 1:\n",
    "X = MinMaxScaler().fit_transform(X_raw)\n",
    "\n",
    "# Shuffle data to remove any underlying pattern that may exist:\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "# Separate training, dev, and test data:\n",
    "test_data, test_labels = X[800000:], y[800000:]\n",
    "dev_data, dev_labels = X[700000:800000], y[700000:800000]\n",
    "train_data, train_labels = X[:700000], y[:700000]\n",
    "\n",
    "mini_train_data, mini_train_labels = X[:75000], y[:75000]\n",
    "mini_dev_data, mini_dev_labels = X[75000:100000], y[75000:100000]\n",
    "labels_set = set(mini_dev_labels)\n",
    "print(labels_set)\n",
    "print(len(labels_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data, version 2, with weather features to improve performance:\n",
    "\n",
    "We seek to add features to our models that will improve performance with respect to out desired performance metric.  There is evidence that there is a correlation between weather patterns and crime, with some experts even arguing for a causal relationship between weather and crime [1].  More specifically, a 2013 paper published in Science showed that higher temperatures and extreme rainfall led to large increases in conflict.  In the setting of strong evidence that weather influences crime, we see it as a candidate for additional features to improve the performance of our classifiers.  Weather data was gathered from (insert source).  Certain features from this data set were incorporated into the original crime data set in order to add features that were hypothesizzed to improve performance.  These features included (insert what we eventually include)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalvi\\AppData\\Local\\conda\\conda\\envs\\py2All1\\lib\\site-packages\\ipykernel_launcher.py:102: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "140303  1431550560                56.0                    70.0   \n",
      "140304  1431554040                57.0                    67.0   \n",
      "140305  1431554160                57.0                    69.0   \n",
      "140306  1431554400                57.0                    69.0   \n",
      "140307  1431557760                57.0                    74.0   \n",
      "140308  1431558900                57.0                    72.0   \n",
      "\n",
      "        HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "140303              0.0                   29.93              10.0   \n",
      "140304              0.0                     NaN              10.0   \n",
      "140305              8.0                   29.92              10.0   \n",
      "140306              8.0                   29.92               NaN   \n",
      "140307              9.0                   29.91              10.0   \n",
      "140308              8.0                     NaN              10.0   \n",
      "\n",
      "        DAILYSunrise  DAILYSunset  \n",
      "140303    1431493260   1431544260  \n",
      "140304    1431493260   1431544260  \n",
      "140305    1431493260   1431544260  \n",
      "140306    1431493260   1431544260  \n",
      "140307    1431493260   1431544260  \n",
      "140308    1431493260   1431544260  \n",
      "              DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "124567  1389459360                54.0                    77.0   \n",
      "124568  1389462960                53.0                    80.0   \n",
      "124569  1389464640                54.0                    77.0   \n",
      "124570  1389465540                52.0                    82.0   \n",
      "124571  1389466560                52.0                    83.0   \n",
      "\n",
      "        HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "124567             14.0                   30.22              10.0   \n",
      "124568              9.0                   30.23              10.0   \n",
      "124569             10.0                     NaN              10.0   \n",
      "124570             11.0                     NaN              10.0   \n",
      "124571              7.0                   30.25              10.0   \n",
      "\n",
      "        DAILYSunrise  DAILYSunset  \n",
      "124567    1389425040   1389460320  \n",
      "124568    1389425040   1389460320  \n",
      "124569    1389425040   1389460320  \n",
      "124570    1389425040   1389460320  \n",
      "124571    1389425040   1389460320  \n",
      "              DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "109394  1348037760                56.0                    77.0   \n",
      "109395  1348041360                57.0                    74.0   \n",
      "109396  1348044960                57.0                    74.0   \n",
      "\n",
      "        HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "109394             14.0                   30.12              10.0   \n",
      "109395              9.0                   30.13              10.0   \n",
      "109396             15.0                   30.14              10.0   \n",
      "\n",
      "        DAILYSunrise  DAILYSunset  \n",
      "109394    1348034100   1348078200  \n",
      "109395    1348034100   1348078200  \n",
      "109396    1348034100   1348078200  \n",
      "             DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "92913  1301896560                50.0                    66.0   \n",
      "92914  1301900160                52.0                    64.0   \n",
      "92915  1301903760                56.0                    55.0   \n",
      "\n",
      "       HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "92913              0.0                   30.17              10.0   \n",
      "92914              0.0                   30.17              10.0   \n",
      "92915              0.0                   30.17              10.0   \n",
      "\n",
      "       DAILYSunrise  DAILYSunset  \n",
      "92913    1301896260   1301942100  \n",
      "92914    1301896260   1301942100  \n",
      "92915    1301896260   1301942100  \n",
      "             DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "77034  1254243360                60.0                    60.0   \n",
      "77035  1254246960                58.0                    67.0   \n",
      "77036  1254250560                57.0                    72.0   \n",
      "\n",
      "       HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "77034             39.0                   29.99              10.0   \n",
      "77035             37.0                   29.99              10.0   \n",
      "77036             25.0                   30.01              10.0   \n",
      "\n",
      "       DAILYSunrise  DAILYSunset  \n",
      "77034    1254204240   1254246840  \n",
      "77035    1254204240   1254246840  \n",
      "77036    1254204240   1254246840  \n",
      "             DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "61113  1209480960                58.0                    60.0   \n",
      "61114  1209484560                57.0                    58.0   \n",
      "61115  1209484800                57.0                    58.0   \n",
      "61116  1209488160                55.0                    64.0   \n",
      "\n",
      "       HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "61113             32.0                   30.07             10.00   \n",
      "61114             33.0                   30.06             10.00   \n",
      "61115             33.0                   30.06              9.94   \n",
      "61116             34.0                   30.05             10.00   \n",
      "\n",
      "       DAILYSunrise  DAILYSunset  \n",
      "61113    1209446100   1209495540  \n",
      "61114    1209446100   1209495540  \n",
      "61115    1209446040   1209495540  \n",
      "61116    1209446040   1209495540  \n",
      "             DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "43577  1162742160                65.0                    73.0   \n",
      "43578  1162742400                65.0                    73.0   \n",
      "43579  1162745760                63.0                    75.0   \n",
      "43580  1162749360                62.0                    78.0   \n",
      "\n",
      "       HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "43577             15.0                   30.20             10.00   \n",
      "43578             15.0                   30.20              9.94   \n",
      "43579             14.0                   30.19             10.00   \n",
      "43580             14.0                   30.21             10.00   \n",
      "\n",
      "       DAILYSunrise  DAILYSunset  \n",
      "43577    1162708740   1162746420  \n",
      "43578    1162708800   1162746360  \n",
      "43579    1162708800   1162746360  \n",
      "43580    1162708800   1162746360  \n",
      "             DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "27369  1118696400                56.0                    81.0   \n",
      "27370  1118699760                55.0                    82.0   \n",
      "27371  1118700000                55.0                    83.0   \n",
      "27372  1118703600                55.0                    83.0   \n",
      "\n",
      "       HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "27369             17.0                   29.86              8.95   \n",
      "27370             13.0                   29.85             10.00   \n",
      "27371             13.0                   29.85              9.94   \n",
      "27372             10.0                   29.85              9.94   \n",
      "\n",
      "       DAILYSunrise  DAILYSunset  \n",
      "27369    1118638020   1118691120  \n",
      "27370    1118638020   1118691120  \n",
      "27371    1118638020   1118691120  \n",
      "27372    1118638020   1118691120  \n",
      "             DATE  HOURLYDRYBULBTEMPF  HOURLYRelativeHumidity  \\\n",
      "11993  1074722400                51.0                    77.0   \n",
      "11994  1074726000                48.0                    68.0   \n",
      "11995  1074729600                49.0                    71.0   \n",
      "\n",
      "       HOURLYWindSpeed  HOURLYSeaLevelPressure  HOURLYVISIBILITY  \\\n",
      "11993              0.0                   30.29               NaN   \n",
      "11994              3.0                   30.30              9.94   \n",
      "11995              0.0                   30.31               NaN   \n",
      "\n",
      "       DAILYSunrise  DAILYSunset  \n",
      "11993    1074669600   1074705720  \n",
      "11994    1074669600   1074705720  \n",
      "11995    1074756000   1074792120  \n",
      "set(['KIDNAPPING', 'WEAPON LAWS', 'SECONDARY CODES', 'WARRANTS', 'PROSTITUTION', 'EMBEZZLEMENT', 'LOITERING', 'FRAUD', 'DRIVING UNDER THE INFLUENCE', 'SEX OFFENSES FORCIBLE', 'ROBBERY', 'BURGLARY', 'SUSPICIOUS OCC', 'FAMILY OFFENSES', 'BRIBERY', 'FORGERY/COUNTERFEITING', 'BAD CHECKS', 'DRUNKENNESS', 'GAMBLING', 'OTHER OFFENSES', 'RECOVERED VEHICLE', 'SUICIDE', 'ARSON', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'NON-CRIMINAL', 'EXTORTION', 'PORNOGRAPHY/OBSCENE MAT', 'LIQUOR LAWS', 'SEX OFFENSES NON FORCIBLE', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'ASSAULT', 'MISSING PERSON', 'DISORDERLY CONDUCT', 'RUNAWAY'])\n",
      "38\n",
      "[[  7.986e-01   3.913e-01   6.667e-01   5.101e-02   1.450e-03   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00\n",
      "    1.000e+00   0.000e+00   0.000e+00   0.000e+00   3.911e-01   7.627e-01\n",
      "    7.186e-02   4.993e-01   9.000e-01   7.987e-01   7.987e-01   1.000e+00]\n",
      " [  6.125e-01   1.000e+00   6.667e-01   5.611e-02   2.273e-04   0.000e+00\n",
      "    1.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   3.799e-01   8.192e-01\n",
      "    2.455e-01   4.913e-01   9.735e-01   6.124e-01   6.124e-01   0.000e+00]\n",
      " [  9.715e-01   2.174e-01   0.000e+00   4.993e-02   1.101e-03   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   1.000e+00   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   1.830e-01   8.249e-01\n",
      "    8.982e-02   8.420e-01   1.000e+00   9.716e-01   9.716e-01   0.000e+00]\n",
      " [  5.213e-01   6.522e-01   1.000e+00   4.562e-02   1.440e-03   0.000e+00\n",
      "    0.000e+00   1.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   5.810e-01   4.162e-01\n",
      "    3.752e-01   5.108e-01   1.000e+00   5.213e-01   5.213e-01   1.000e+00]\n",
      " [  4.211e-01   7.391e-01   3.333e-01   5.120e-02   1.205e-03   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00   1.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   3.665e-01   7.220e-01\n",
      "    3.784e-01   6.753e-01   9.988e-01   4.211e-01   4.211e-01   1.000e+00]\n",
      " [  5.076e-01   9.565e-01   0.000e+00   4.213e-02   1.232e-03   0.000e+00\n",
      "    0.000e+00   1.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   2.584e-01   6.893e-01\n",
      "    4.790e-01   5.108e-01   9.985e-01   5.075e-01   5.075e-01   0.000e+00]\n",
      " [  7.153e-01   5.217e-01   1.000e+00   5.048e-02   1.475e-03   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00\n",
      "    1.000e+00   0.000e+00   0.000e+00   0.000e+00   3.925e-01   5.932e-01\n",
      "    2.635e-01   5.606e-01   9.985e-01   7.153e-01   7.153e-01   1.000e+00]\n",
      " [  3.559e-01   5.217e-01   3.333e-01   4.487e-02   6.184e-04   0.000e+00\n",
      "    1.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   4.511e-01   5.650e-01\n",
      "    3.653e-01   5.909e-01   9.985e-01   3.559e-01   3.559e-01   1.000e+00]\n",
      " [  7.701e-01   8.261e-01   1.667e-01   1.188e-02   4.986e-04   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   1.000e+00   4.721e-01   6.017e-01\n",
      "    5.269e-01   3.810e-01   1.000e+00   7.701e-01   7.701e-01   0.000e+00]\n",
      " [  4.562e-01   7.826e-01   1.000e+00   5.475e-02   1.292e-03   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   0.000e+00   1.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00   0.000e+00   5.852e-01   6.271e-01\n",
      "    6.168e-01   3.810e-01   9.985e-01   4.561e-01   4.561e-01   1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/train_transformed.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "x_data = df.drop('category', 1)\n",
    "y = df.category.as_matrix()\n",
    "\n",
    "########## Adding the date back into the data\n",
    "import csv\n",
    "import time\n",
    "import calendar\n",
    "data_path = \"./data/train.csv\"\n",
    "dataCSV = open(data_path, 'rt')\n",
    "csvData = list(csv.reader(dataCSV))\n",
    "csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "allData = csvData[1:]\n",
    "dataCSV.close()\n",
    "\n",
    "df2 = pd.DataFrame(allData)\n",
    "df2.columns = csvFields\n",
    "dates = df2['Dates']\n",
    "dates = dates.apply(time.strptime, args=(\"%Y-%m-%d %H:%M:%S\",))\n",
    "dates = dates.apply(calendar.timegm)\n",
    "#print(dates.head())\n",
    "\n",
    "x_data['secondsFromEpoch'] = dates\n",
    "colnames = x_data.columns.tolist()\n",
    "colnames = colnames[-1:] + colnames[:-1]\n",
    "x_data = x_data[colnames]\n",
    "##########\n",
    "\n",
    "########## Adding the weather data into the original crime data\n",
    "weatherData1 = \"./data/1027175.csv\"\n",
    "weatherData2 = \"./data/1027176.csv\"\n",
    "dataCSV = open(weatherData1, 'rt')\n",
    "csvData = list(csv.reader(dataCSV))\n",
    "csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "allWeatherData1 = csvData[1:]\n",
    "dataCSV.close()\n",
    "\n",
    "dataCSV = open(weatherData2, 'rt')\n",
    "csvData = list(csv.reader(dataCSV))\n",
    "csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "allWeatherData2 = csvData[1:]\n",
    "dataCSV.close()\n",
    "\n",
    "weatherDF1 = pd.DataFrame(allWeatherData1)\n",
    "weatherDF1.columns = csvFields\n",
    "dates1 = weatherDF1['DATE']\n",
    "sunrise1 = weatherDF1['DAILYSunrise']\n",
    "sunset1 = weatherDF1['DAILYSunset']\n",
    "\n",
    "weatherDF2 = pd.DataFrame(allWeatherData2)\n",
    "weatherDF2.columns = csvFields\n",
    "dates2 = weatherDF2['DATE']\n",
    "sunrise2 = weatherDF2['DAILYSunrise']\n",
    "sunset2 = weatherDF2['DAILYSunset']\n",
    "\n",
    "#functions for processing the sunrise and sunset times of each day\n",
    "def get_hour_and_minute(milTime):\n",
    "    hour = int(milTime[:-2])\n",
    "    minute = int(milTime[-2:])\n",
    "    return [hour, minute]\n",
    "\n",
    "def get_date_only(date):\n",
    "    return time.struct_time(tuple([date[0], date[1], date[2], 0, 0, 0, date[6], date[7], date[8]]))\n",
    "\n",
    "def structure_sun_time(timeSeries, dateSeries):\n",
    "    sunTimes = timeSeries.copy()\n",
    "    for index in range(len(dateSeries)):\n",
    "        sunTimes[index] = time.struct_time(tuple([dateSeries[index][0], dateSeries[index][1], dateSeries[index][2], timeSeries[index][0], timeSeries[index][1], dateSeries[index][5], dateSeries[index][6], dateSeries[index][7], dateSeries[index][8]]))\n",
    "    return sunTimes\n",
    "\n",
    "dates1 = dates1.apply(time.strptime, args=(\"%Y-%m-%d %H:%M\",))\n",
    "sunrise1 = sunrise1.apply(get_hour_and_minute)\n",
    "sunrise1 = structure_sun_time(sunrise1, dates1)\n",
    "sunrise1 = sunrise1.apply(calendar.timegm)\n",
    "sunset1 = sunset1.apply(get_hour_and_minute)\n",
    "sunset1 = structure_sun_time(sunset1, dates1)\n",
    "sunset1 = sunset1.apply(calendar.timegm)\n",
    "dates1 = dates1.apply(calendar.timegm)\n",
    "\n",
    "dates2 = dates2.apply(time.strptime, args=(\"%Y-%m-%d %H:%M\",))\n",
    "sunrise2 = sunrise2.apply(get_hour_and_minute)\n",
    "sunrise2 = structure_sun_time(sunrise2, dates2)\n",
    "sunrise2 = sunrise2.apply(calendar.timegm)\n",
    "sunset2 = sunset2.apply(get_hour_and_minute)\n",
    "sunset2 = structure_sun_time(sunset2, dates2)\n",
    "sunset2 = sunset2.apply(calendar.timegm)\n",
    "dates2 = dates2.apply(calendar.timegm)\n",
    "\n",
    "weatherDF1['DATE'] = dates1\n",
    "weatherDF1['DAILYSunrise'] = sunrise1\n",
    "weatherDF1['DAILYSunset'] = sunset1\n",
    "weatherDF2['DATE'] = dates2\n",
    "weatherDF2['DAILYSunrise'] = sunrise2\n",
    "weatherDF2['DAILYSunset'] = sunset2\n",
    "\n",
    "weatherDF = pd.concat([weatherDF1,weatherDF2[32:]],ignore_index=True)\n",
    "\n",
    "# Starting off with some of the easier features to work with-- more to come here . . . still in beta\n",
    "weatherMetrics = weatherDF[['DATE','HOURLYDRYBULBTEMPF','HOURLYRelativeHumidity', 'HOURLYWindSpeed', \\\n",
    "                            'HOURLYSeaLevelPressure', 'HOURLYVISIBILITY', 'DAILYSunrise', 'DAILYSunset']]\n",
    "weatherMetrics = weatherMetrics.convert_objects(convert_numeric=True)\n",
    "weatherDates = weatherMetrics['DATE']\n",
    "#'DATE','HOURLYDRYBULBTEMPF','HOURLYRelativeHumidity', 'HOURLYWindSpeed',\n",
    "#'HOURLYSeaLevelPressure', 'HOURLYVISIBILITY'\n",
    "timeWindow = 10800 #3 hours\n",
    "hourlyDryBulbTemp = []\n",
    "hourlyRelativeHumidity = []\n",
    "hourlyWindSpeed = []\n",
    "hourlySeaLevelPressure = []\n",
    "hourlyVisibility = []\n",
    "dailySunrise = []\n",
    "dailySunset = []\n",
    "daylight = []\n",
    "test = 0\n",
    "for timePoint in dates:#dates is the epoch time from the kaggle data\n",
    "    relevantWeather = weatherMetrics[(weatherDates <= timePoint) & (weatherDates > timePoint - timeWindow)]\n",
    "    hourlyDryBulbTemp.append(relevantWeather['HOURLYDRYBULBTEMPF'].mean())\n",
    "    hourlyRelativeHumidity.append(relevantWeather['HOURLYRelativeHumidity'].mean())\n",
    "    hourlyWindSpeed.append(relevantWeather['HOURLYWindSpeed'].mean())\n",
    "    hourlySeaLevelPressure.append(relevantWeather['HOURLYSeaLevelPressure'].mean())\n",
    "    hourlyVisibility.append(relevantWeather['HOURLYVISIBILITY'].mean())\n",
    "    dailySunrise.append(relevantWeather['DAILYSunrise'].iloc[-1])\n",
    "    dailySunset.append(relevantWeather['DAILYSunset'].iloc[-1])\n",
    "    daylight.append(1.0*((timePoint >= relevantWeather['DAILYSunrise'].iloc[-1]) and (timePoint < relevantWeather['DAILYSunset'].iloc[-1])))\n",
    "    #if timePoint < relevantWeather['DAILYSunset'][-1]:\n",
    "        #daylight.append(1)\n",
    "    #else:\n",
    "        #daylight.append(0)\n",
    "    \n",
    "    if test%100000 == 0:\n",
    "        print(relevantWeather)\n",
    "    test += 1\n",
    "\n",
    "hourlyDryBulbTemp = pd.Series.from_array(np.array(hourlyDryBulbTemp))\n",
    "hourlyRelativeHumidity = pd.Series.from_array(np.array(hourlyRelativeHumidity))\n",
    "hourlyWindSpeed = pd.Series.from_array(np.array(hourlyWindSpeed))\n",
    "hourlySeaLevelPressure = pd.Series.from_array(np.array(hourlySeaLevelPressure))\n",
    "hourlyVisibility = pd.Series.from_array(np.array(hourlyVisibility))\n",
    "dailySunrise = pd.Series.from_array(np.array(dailySunrise))\n",
    "dailySunset = pd.Series.from_array(np.array(dailySunset))\n",
    "daylight = pd.Series.from_array(np.array(daylight))\n",
    "\n",
    "x_data['HOURLYDRYBULBTEMPF'] = hourlyDryBulbTemp\n",
    "x_data['HOURLYRelativeHumidity'] = hourlyRelativeHumidity\n",
    "x_data['HOURLYWindSpeed'] = hourlyWindSpeed\n",
    "x_data['HOURLYSeaLevelPressure'] = hourlySeaLevelPressure\n",
    "x_data['HOURLYVISIBILITY'] = hourlyVisibility\n",
    "x_data['DAILYSunrise'] = dailySunrise\n",
    "x_data['DAILYSunset'] = dailySunset\n",
    "x_data['Daylight'] = daylight\n",
    "\n",
    "x_data.to_csv(path_or_buf=\"C:/MIDS/W207 final project/x_data.csv\")\n",
    "##########\n",
    "\n",
    "# Impute missing values with mean values:\n",
    "x_complete = x_data.fillna(x_data.mean())\n",
    "X_raw = x_complete.as_matrix()\n",
    "\n",
    "# Scale the data between 0 and 1:\n",
    "X = MinMaxScaler().fit_transform(X_raw)\n",
    "\n",
    "# Shuffle data to remove any underlying pattern that may exist:\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "# Separate training, dev, and test data:\n",
    "test_data, test_labels = X[800000:], y[800000:]\n",
    "dev_data, dev_labels = X[700000:800000], y[700000:800000]\n",
    "train_data, train_labels = X[:700000], y[:700000]\n",
    "\n",
    "mini_train_data, mini_train_labels = X[:75000], y[:75000]\n",
    "mini_dev_data, mini_dev_labels = X[75000:100000], y[75000:100000]\n",
    "labels_set = set(mini_dev_labels)\n",
    "print(labels_set)\n",
    "print(len(labels_set))\n",
    "print(train_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting to meet Kaggle submission standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Kaggle submission format requires listing the ID of each example.\n",
    "# This is to remember the order of the IDs after shuffling\n",
    "allIDs = np.array(list(df.axes[0]))\n",
    "allIDs = allIDs[shuffle]\n",
    "\n",
    "testIDs = allIDs[800000:]\n",
    "devIDs = allIDs[700000:800000]\n",
    "trainIDs = allIDs[:700000]\n",
    "\n",
    "# Extract the column names for the required submission format\n",
    "sampleSubmission_path = \"./data/sampleSubmission.csv\"\n",
    "sampleDF = pd.read_csv(sampleSubmission_path)\n",
    "allColumns = list(sampleDF.columns)\n",
    "featureColumns = allColumns[1:]\n",
    "\n",
    "# Extracting the test data for a baseline submission\n",
    "real_test_path = \"./data/test_transformed.csv\"\n",
    "testDF = pd.read_csv(real_test_path, header=0)\n",
    "real_test_data = testDF\n",
    "\n",
    "test_complete = real_test_data.fillna(real_test_data.mean())\n",
    "Test_raw = test_complete.as_matrix()\n",
    "\n",
    "TestData = MinMaxScaler().fit_transform(Test_raw)\n",
    "\n",
    "# Here we remember the ID of each test data point, in case we ever decide to shuffle the test data for some reason\n",
    "testIDs = list(testDF.axes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate baseline prediction probabilities from MNB classifier and store in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a baseline MNB classifier and make it return prediction probabilities for the actual test data\n",
    "def MNB():\n",
    "    mnb = MultinomialNB(alpha = 0.0000001)\n",
    "    mnb.fit(train_data, train_labels)\n",
    "    #print(\"\\n\\nMultinomialNB accuracy on dev data:\", mnb.score(dev_data, dev_labels))\n",
    "    return mnb.predict_proba(real_test_data)\n",
    "MNB()\n",
    "\n",
    "baselinePredictionProbabilities = MNB()\n",
    "\n",
    "# Place the resulting prediction probabilities in a .csv file in the required format\n",
    "# First, turn the prediction probabilties into a data frame\n",
    "resultDF = pd.DataFrame(baselinePredictionProbabilities,columns=featureColumns)\n",
    "# Add the IDs as a final column\n",
    "resultDF.loc[:,'Id'] = pd.Series(testIDs,index=resultDF.index)\n",
    "# Make the 'Id' column the first column\n",
    "colnames = resultDF.columns.tolist()\n",
    "colnames = colnames[-1:] + colnames[:-1]\n",
    "resultDF = resultDF[colnames]\n",
    "# Output to a .csv file\n",
    "resultDF.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the code above will shuffle data differently every time it's run, so model accuracies will vary accordingly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.391e-01   6.667e-01   6.117e-02   9.047e-04   1.000e+00   0.000e+00\n",
      "    0.000e+00   0.000e+00   1.000e+00   0.000e+00   0.000e+00   0.000e+00\n",
      "    0.000e+00   0.000e+00   0.000e+00]]\n",
      "['WEAPON LAWS']\n"
     ]
    }
   ],
   "source": [
    "## Data sub-setting quality check-point\n",
    "print(train_data[:1])\n",
    "print(train_labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MultinomialNB accuracy on dev data: 0.22314\n"
     ]
    }
   ],
   "source": [
    "# Modeling quality check-point with MNB--fast model\n",
    "\n",
    "def MNB():\n",
    "    mnb = MultinomialNB(alpha = 0.0000001)\n",
    "    mnb.fit(train_data, train_labels)\n",
    "    print(\"\\n\\nMultinomialNB accuracy on dev data:\", mnb.score(dev_data, dev_labels))\n",
    "    \n",
    "MNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Performance Criteria\n",
    "\n",
    "As determined by the Kaggle submission guidelines, the performance criteria metric for the San Francisco Crime Classification competition is Multi-class Logarithmic Loss (also known as cross-entropy).  There are various other performance metrics that are appropriate for different domains: accuracy, F-score, Lift, ROC Area, average precision, precision/recall break-even point, and squared error.\n",
    "\n",
    "(Describe each performance metric and a domain in which it is preferred. Give Pros/Cons if able)\n",
    "\n",
    "- Multi-class Log Loss:\n",
    "\n",
    "- Accuracy:\n",
    "\n",
    "- F-score:\n",
    "\n",
    "- Lift:\n",
    "\n",
    "- ROC Area:\n",
    "\n",
    "- Average precision\n",
    "\n",
    "- Precision/Recall break-even point:\n",
    "\n",
    "- Squared-error:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prototyping\n",
    "We will start our classifier and feature engineering process by looking at the performance of various classifiers with default parameter settings in predicting labels on the mini_dev_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_prototype(train_data, train_labels, eval_data, eval_labels):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5).fit(train_data, train_labels)\n",
    "    bnb = BernoulliNB(alpha=1, binarize = 0.5).fit(train_data, train_labels)\n",
    "    mnb = MultinomialNB().fit(train_data, train_labels)\n",
    "    log_reg = LogisticRegression().fit(train_data, train_labels)\n",
    "    support_vm = svm.SVC().fit(train_data, train_labels)\n",
    "    neural_net = MLPClassifier().fit(train_data, train_labels)\n",
    "    random_forest = RandomForestClassifier().fit(train_data, train_labels)\n",
    "    decision_tree = DecisionTreeClassifier().fit(train_data, train_labels)\n",
    "    \n",
    "    models = [knn, bnb, mnb, log_reg, support_vm, neural_net, random_forest, decision_tree]\n",
    "    for model in models:\n",
    "        eval_prediction_probabilities = model.predict_proba(eval_data)\n",
    "        eval_predictions = model.predict(eval_data)\n",
    "        set_eval_predictions = set(eval_predictions)\n",
    "        crime_labels = list(set_eval_predictions)\n",
    "        print(model, \"Multi-class Log Loss:\", log_loss(y_true = eval_labels, y_pred = eval_prediction_probabilities, labels = crime_labels), \"\\n\\n\")\n",
    "\n",
    "# model_prototype(mini_train_data, mini_train_labels, mini_dev_data, mini_dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features, Hyperparameter Tuning, and Model Calibration To Improve Prediction For Each Classifier\n",
    "\n",
    "Here we seek to optimize the performance of our classifiers in a three-step, dynamnic engineering process. \n",
    "\n",
    "##### 1) Feature addition\n",
    "\n",
    "We previously added components from the weather data into the original SF crime data as new features.  Here, we will incorporate those features into our classifiers to determine whether or not they improve performance as hypothesized.\n",
    "\n",
    "##### 2) Hyperparameter tuning\n",
    "\n",
    "Each classifier has parameters that we can engineer to further optimize performance, as opposed to using the default parameter values as we did above in the model prototyping cell. (Will likely use pipeline here in future)\n",
    "\n",
    "##### 3) Model calibration\n",
    "\n",
    "We can calibrate the models via Platt Scaling or Isotonic Regression to attempt to improve their performance.\n",
    "\n",
    "- Platt Scaling: (brief explanation of how it works)\n",
    "\n",
    "- Isotonic Regression: ((brief explanation of how it works))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Addition\n",
    "\n",
    "(Decide whether to do this as one big step, or to engineer the addition of individual features for individual classifiers.  Can likely do all feature addition here, continuing on the work done with the weather data import completed in earlier cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Feature addition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the KNN classifier, we can seek to optimize the following classifier parameters: n-neighbors, weights, and the power parameter ('p')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def k_neighbors_tuned():\n",
    "    \n",
    "    k_value_tuning = [i for i in range(1,100,2)]\n",
    "    weight_tuning = ['uniform', 'distance']\n",
    "    power_parameter_tuning = [1,2]\n",
    "    for k in k_value_tuning:\n",
    "        for w in weight_tuning:\n",
    "            for p in power_parameter_tuning:\n",
    "                tuned_KNN = KNeighborsClassifier(n_neighbors=k, weights=w, p=p).fit(train_data, train_labels)\n",
    "                dev_preds = tuned_KNN.predict(dev_data)\n",
    "                set_dev_preds = set(dev_preds)\n",
    "                crime_labels_tuned = list(set_dev_preds)\n",
    "                dev_prediction_probabilities = tuned_KNN.predict_proba(dev_data)\n",
    "                print(tuned_KNN, \"Multi-class Log Loss:\", log_loss(y_true = dev_labels, y_pred = dev_prediction_probabilities, labels = crime_labels_tuned), \"\\n\\n\")\n",
    "\n",
    "# k_neighbors_tuned()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model calibration:\n",
    "\n",
    "We will consider embeding this step within the for loop for the hyperparameter tuning. More likely we will pipeline it along with the hyperparameter tuning steps.  We will then use GridSearchCV top find the optimized parameters based on our performance metric of Mutli-Class Log Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def k_calibrated():\n",
    "\n",
    "# Here we will calibrate the KNN classifier with both Platt Scaling and with Isotonic Regression using CalibratedClassifierCV\n",
    "# with various parameter settings.  The \"method\" parameter can be set to \"sigmoid\" or to \"isotonic\", \n",
    "# corresponding to Platt Scaling and to Isotonic Regression respectively.\n",
    "\n",
    "    methods = ['signoid', 'isotonic']\n",
    "    for m in methods:\n",
    "        CCV_for_KNN = CalibratedClassifierCV()\n",
    "        \n",
    "        # Will likely embed this step within the for loop for the hyperparameter tuning as that makes more sense. #\n",
    "        # Or will pipeline it along with the hyperparameter tuning steps #\n",
    "\n",
    "# k_calibrated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial, Bernoulli, and Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GNB():\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train_data, train_labels)\n",
    "    print(\"GaussianNB accuracy on dev data:\", \n",
    "          gnb.score(dev_data, dev_labels))\n",
    "    \n",
    "    # Gaussian Naive Bayes requires the data to have a relative normal distribution. Sometimes\n",
    "    # adding noise can improve performance by making the data more normal:\n",
    "    train_data_noise = np.random.rand(train_data.shape[0],train_data.shape[1])\n",
    "    modified_train_data = np.multiply(train_data,train_data_noise)    \n",
    "    gnb_noise = GaussianNB()\n",
    "    gnb.fit(modified_train_data, train_labels)\n",
    "    print(\"GaussianNB accuracy with added noise:\", \n",
    "          gnb.score(dev_data, dev_labels))    \n",
    "    \n",
    "# Going slightly deeper with hyperparameter tuning and model calibration:\n",
    "def BNB(alphas):\n",
    "    \n",
    "    bnb_one = BernoulliNB(binarize = 0.5)\n",
    "    bnb_one.fit(train_data, train_labels)\n",
    "    print(\"\\n\\nBernoulli Naive Bayes accuracy when alpha = 1 (the default value):\",\n",
    "          bnb_one.score(dev_data, dev_labels))\n",
    "    \n",
    "    bnb_zero = BernoulliNB(binarize = 0.5, alpha=0)\n",
    "    bnb_zero.fit(train_data, train_labels)\n",
    "    print(\"BNB accuracy when alpha = 0:\", bnb_zero.score(dev_data, dev_labels))\n",
    "    \n",
    "    bnb = BernoulliNB(binarize=0.5)\n",
    "    clf = GridSearchCV(bnb, param_grid = alphas)\n",
    "    clf.fit(train_data, train_labels)\n",
    "    print(\"Best parameter for BNB on the dev data:\", clf.best_params_)\n",
    "    \n",
    "    clf_tuned = BernoulliNB(binarize = 0.5, alpha=0.00000000000000000000001)\n",
    "    clf_tuned.fit(train_data, train_labels)\n",
    "    print(\"Accuracy using the tuned Laplace smoothing parameter:\", \n",
    "          clf_tuned.score(dev_data, dev_labels), \"\\n\\n\")\n",
    "    \n",
    "\n",
    "def investigate_model_calibration(buckets, correct, total):\n",
    "    clf_tuned = BernoulliNB(binarize = 0.5, alpha=0.00000000000000000000001)\n",
    "    clf_tuned.fit(train_data, train_labels)\n",
    "    \n",
    "    # Establish data sets\n",
    "    pred_probs = clf_tuned.predict_proba(dev_data)\n",
    "    max_pred_probs = np.array(pred_probs.max(axis=1))\n",
    "    preds = clf_tuned.predict(dev_data)\n",
    "        \n",
    "    # For each bucket, look at the predictions that the model yields. \n",
    "    # Keep track of total & correct predictions within each bucket.\n",
    "    bucket_bottom = 0\n",
    "    bucket_top = 0\n",
    "    for bucket_index, bucket in enumerate(buckets):\n",
    "        bucket_top = bucket\n",
    "        for pred_index, pred in enumerate(preds):\n",
    "            if (max_pred_probs[pred_index] <= bucket_top) and (max_pred_probs[pred_index] > bucket_bottom):\n",
    "                total[bucket_index] += 1\n",
    "                if preds[pred_index] == dev_labels[pred_index]:\n",
    "                    correct[bucket_index] += 1\n",
    "        bucket_bottom = bucket_top\n",
    "\n",
    "def MNB():\n",
    "    mnb = MultinomialNB(alpha = 0.0000001)\n",
    "    mnb.fit(train_data, train_labels)\n",
    "    print(\"\\n\\nMultinomialNB accuracy on dev data:\", mnb.score(dev_data, dev_labels))\n",
    "\n",
    "alphas = {'alpha': [0.00000000000000000000001, 0.0000001, 0.0001, 0.001, \n",
    "                    0.01, 0.1, 0.0, 0.5, 1.0, 2.0, 10.0]}\n",
    "buckets = [0.5, 0.9, 0.99, 0.999, .9999, 0.99999, 1.0]\n",
    "correct = [0 for i in buckets]\n",
    "total = [0 for i in buckets]\n",
    "\n",
    "MNB()\n",
    "GNB()\n",
    "BNB(alphas)\n",
    "investigate_model_calibration(buckets, correct, total)\n",
    "\n",
    "for i in range(len(buckets)):\n",
    "   accuracy = 0.0\n",
    "   if (total[i] > 0): accuracy = correct[i] / total[i]\n",
    "   print('p(pred) <= %.13f    total = %3d    accuracy = %.3f' %(buckets[i], total[i], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli Naive Bayes and Multinomial Naive Bayes models can predict whether a loan will be good or bad with XXX% accuracy.\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "We will prune the work above.  Will seek to optimize the alpha parameter (Laplace smoothing parameter) for MNB and BNB classifiers.\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "Here we will calibrate the MNB, BNB and GNB classifiers with both Platt Scaling and with Isotonic Regression using CalibratedClassifierCV with various parameter settings.  The \"method\" parameter can be set to \"sigmoid\" or to \"isotonic\", corresponding to Platt Scaling and to Isotonic Regression respectively.  Will likely embed this step within the for loop for the hyperparameter tuning as that makes more sense. Or will pipeline it along with the hyperparameter tuning steps.  We will then use GridSearchCV top find the optimized parameters based on our performance metric of Mutli-Class Log Loss.\n",
    "\n",
    "THE REST OF THE MODEL CALIBRATION SECTIONS ARE SIMILAR AND THE OUTLINE WILL NOT BE REPEATED AS WOULD BE REDUNDANT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Logistic Regression classifier, we can seek to optimize the following classifier parameters: penalty (l1 or l2), C (inverse of regularization strength), solver ('newton-cg', 'lbfgs', 'liblinear', or 'sag')\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Decision Tree classifier, we can seek to optimize the following classifier parameters: min_samples_leaf (the minimum number of samples required to be at a leaf node), max_depth\n",
    "\n",
    "From readings, setting min_samples_leaf to approximately 1% of the data points can stop the tree from inappropriately classifying outliers, which can help to improve accuracy (unsure if significantly improves MCLL).\n",
    "\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the SVM classifier, we can seek to optimize the following classifier parameters: C (penalty parameter C of the error term), kernel ('linear', 'poly', 'rbf', sigmoid', or 'precomputed')\n",
    "\n",
    "See source [2] for parameter optimization in SVM\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Neural Networks MLP classifier, we can seek to optimize the following classifier parameters: hidden_layer_sizes, activation ('identity', 'logistic', 'tanh', 'relu'), solver ('lbfgs','sgd', adam'), alpha, learning_rate ('constant', 'invscaling','adaptive')\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Random Forest classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_features, max_depth, min_samples_leaf, bootstrap (whether or not bootstrap samples are used when building trees), oob_score (whether or not out-of-bag samples are used to estimate the generalization accuracy)\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "There are no major changes that we seek to make in the AdaBoostClassifier with respect to default parameter values.\n",
    "\n",
    "###### Adaboosting each classifier:\n",
    "\n",
    "We will run the AdaBoostClassifier on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Bagging meta classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_samples, max_features, bootstrap (whether or not bootstrap samples are used when building trees), bootstrap_features (whether features are drawn with replacement), and oob_score (whether or not out-of-bag samples are used to estimate the generalization accuracy)\n",
    "\n",
    "###### Bagging each classifier:\n",
    "\n",
    "We will run the BaggingClassifier on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Gradient Boosting meta classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_depth, min_samples_leaf, and max_features\n",
    "\n",
    "###### Gradient Boosting each classifier:\n",
    "\n",
    "We will run the GradientBoostingClassifier with loss = 'deviance' (as loss = 'exponential' uses the AdaBoost algorithm) on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we will likely use Pipeline and GridSearchCV in order to find the overall classifier with optimized Multi-class Log Loss.\n",
    "# This will be the last step after all attempts at feature addition, hyperparameter tuning, and calibration are completed\n",
    "# and the corresponding performance metrics are gathered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1) Hsiang, Solomon M. and Burke, Marshall and Miguel, Edward. \"Quantifying the Influence of Climate on Human Conflict\". Science, Vol 341, Issue 6151, 2013   \n",
    "\n",
    "2) Huang, Cheng-Lung. Wang, Chieh-Jen. \"A GA-based feature selection and parameters optimization for support vector machines\". Expert Systems with Applications, Vol 31, 2006, p 231-240\n",
    "\n",
    "3) More to come \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
