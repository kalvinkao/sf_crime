{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle San Francisco Crime Classification\n",
    "## Berkeley MIDS W207 Final Project\n",
    "### Sarah Cha, Sam Goodgame, Kalvin Kao, Bryan Moore\n",
    "\n",
    "**This project attempts to predict the type of crime commited in San Francisco based on known attributes of the crime. It is part of a [Kaggle competition](https://www.kaggle.com/c/sf-crime).**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task in this project is to predict the type of a crime based on its component details. The component details are primarily related to time and location.\n",
    "\n",
    "This is an interesting problem space because time and location are both high-dimensional variables. Such variables don't tend to work well with machine learning models, because they lead the models to overfit and generalize poorly. \n",
    "\n",
    "Accordingly, our goal in this project is to generate an accurate, parsimonious model by working our way through the model selection triple:\n",
    "- Model selection\n",
    "- Feature Engineering\n",
    "- Hyperparameter tuning (and calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling libraries and estimators\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Meta-estimators\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Calibration tools\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Set random seed and format print output\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step was to transform the data into a form that we could use for machine learning: a Numpy array with a single column of targets (in this case, the type of crime committed) and many columns of independent/predictor variables.\n",
    "\n",
    "There are many ways to wrangle the data; we chose SQL for the first step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use DDL below to construct a table for the data within a PostgreSQL database:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE kaggle_sf_crime (\n",
    "dates TIMESTAMP,                                \n",
    "category VARCHAR,\n",
    "descript VARCHAR,\n",
    "dayofweek VARCHAR,\n",
    "pd_district VARCHAR,\n",
    "resolution VARCHAR,\n",
    "addr VARCHAR,\n",
    "X FLOAT,\n",
    "Y FLOAT);\n",
    "```\n",
    "#### Move the training data, downloaded from Kaggle as a .csv file, into the PostgreSQL database:\n",
    "```sql\n",
    "\\copy kaggle_sf_crime FROM '/Users/Goodgame/Desktop/MIDS/207/final/sf_crime_train.csv' DELIMITER ',' CSV HEADER;\n",
    "```\n",
    "\n",
    "#### Use SQL to transform the data into a usable form:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  category,\n",
    "  date_part('hour', dates) AS hour_of_day,\n",
    "  CASE\n",
    "    WHEN dayofweek = 'Monday' then 1\n",
    "    WHEN dayofweek = 'Tuesday' THEN 2\n",
    "    WHEN dayofweek = 'Wednesday' THEN 3\n",
    "    WHEN dayofweek = 'Thursday' THEN 4\n",
    "    WHEN dayofweek = 'Friday' THEN 5\n",
    "    WHEN dayofweek = 'Saturday' THEN 6\n",
    "    WHEN dayofweek = 'Sunday' THEN 7\n",
    "  END AS dayofweek_numeric,\n",
    "  X,\n",
    "  Y,\n",
    "  CASE\n",
    "    WHEN pd_district = 'BAYVIEW' THEN 1\n",
    "    ELSE 0\n",
    "  END AS bayview_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'INGLESIDE' THEN 1\n",
    "    ELSE 0\n",
    "  END AS ingleside_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'NORTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS northern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'CENTRAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS central_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'MISSION' THEN 1\n",
    "    ELSE 0\n",
    "  END AS mission_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'SOUTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS southern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TENDERLOIN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS tenderloin_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'PARK' THEN 1\n",
    "    ELSE 0\n",
    "  END AS park_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'RICHMOND' THEN 1\n",
    "    ELSE 0\n",
    "  END AS richmond_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TARAVAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS taraval_binary\n",
    "FROM kaggle_sf_crime;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "#### Location One-Hot Encoding\n",
    "Location information tends to be extremely high-dimensional, which presents difficulties for modeling. The original dataset's location information came in three major formats: \n",
    "1. X and Y coordinates\n",
    "2. Street address information\n",
    "3. Police department districts\n",
    "\n",
    "After visualizing the data and conducting basic exploratory data analysis, we decided to use one-hot encoding the transform the police department location information into features. In other words, each police department becomes a feature, and a given observation receives a value of '1' if it occured in the police department described by the feature. Otherwise, it received a value of '0'. Approaching location in this way allowed us to preserve the parsimony of our model; it retains the majority of the important variance in the data without overfitting. \n",
    "\n",
    "#### Time One-Hot Encoding\n",
    "We took the same approach to simplifying time data. Time is also high-dimensional; if we were to try to model crimes based on the provided timestamps (which include seconds), then the eight weeks of data generate 4,838,400 possibilities. We used one-hot encoding to simplify those possibilities into one 7-dimensional day-of-week feature, and an addition 24-dimensional hour-of-day feature. We beleive that these features capture the most variation from the data and allow our models to generalize without risking overfitting.\n",
    "\n",
    "### Weather Data (Kalvin Add Here)\n",
    "We sought to add features to our models that improved performance with respect to out desired performance metric.  Scientists before us have documented correlations between weather patterns and crime; some experts even argue for a causal relationship between weather and crime [1].  More specifically, a 2013 paper published in *Science* demonstrated that higher temperatures and extreme rainfall led to large increases in conflict.  Due to this research, we see weather as a source for additional features to improve the performance of our classifiers.  We gathered weather data from the National Centers for Environmental Information --specifically, teh National Oceanic and Atmospheric Administration.\n",
    "\n",
    "We created the following weather features: <<<<INSERT FEATURES>>>>>\n",
    "\n",
    "<<<<RATIONALE FOR WEATHER FEATURES>>>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "Once we generated our features, we tranformed them into the appropriate formats. We also broke the data into subsets for training and development. We also created smaller subsets for prototyping quickly.\n",
    "\n",
    "One point of note: we removed two categories from the data. The crimes labelled 'TREA' (treason) and 'PORNOGRAPHY/OBSCENE MAT' occurred so infrequently that they presented difficulties for modeling. The best approach was simply to remove them. We assume the risk of incorrectly identifying these crimes in the test set, but doing so allows us to proceed with modeling unhindered by off-by-one errors.\n",
    "\n",
    "The final CSV we use for modeling is [here](https://drive.google.com/file/d/0B74-LZykH7Cud0hmYXBjNzZCaEU/view?ts=5998bc78)--to follow along with our code, download it and adjust the first line of the code below to reference wherever it's stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There should be 37 of each type of label -- otherwise, we'll have an issue later:\n",
      "37 37 37 37\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/Goodgame/Desktop/prototyping/x_data_3.csv\"\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "x_data = df.drop('category', 1)\n",
    "y = df.category.as_matrix()\n",
    "\n",
    "# Impute missing values with mean values:\n",
    "x_complete = x_data.fillna(x_data.mean())\n",
    "X_raw = x_complete.as_matrix()\n",
    "\n",
    "# Scale the data between 0 and 1:\n",
    "X = MinMaxScaler().fit_transform(X_raw)\n",
    "\n",
    "# Shuffle data to remove any underlying pattern that may exist\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "'''Due to difficulties with log loss and set(y_pred) needing to match set(labels), \n",
    "we will remove the extremely rare crimes from the data for quality issues.'''\n",
    "X_minus_trea = X[np.where(y != 'TREA')]\n",
    "y_minus_trea = y[np.where(y != 'TREA')]\n",
    "X_final = X_minus_trea[np.where(y_minus_trea != 'PORNOGRAPHY/OBSCENE MAT')]\n",
    "y_final = y_minus_trea[np.where(y_minus_trea != 'PORNOGRAPHY/OBSCENE MAT')]\n",
    "\n",
    "# Separate training, dev, and test data:\n",
    "all_train_data, all_train_labels = X, y\n",
    "dev_data, dev_labels = X_final[700000:], y_final[700000:]\n",
    "train_data, train_labels = X_final[100000:700000], y_final[100000:700000]\n",
    "calibrate_data, calibrate_labels = X_final[:100000], y_final[:100000]\n",
    "\n",
    "# Mini datasets for quick prototyping\n",
    "mini_train_data, mini_train_labels = X_final[:20000], y_final[:20000]\n",
    "mini_calibrate_data, mini_calibrate_labels = X_final[19000:28000], y_final[19000:28000]\n",
    "mini_dev_data, mini_dev_labels = X_final[49000:60000], y_final[49000:60000]\n",
    "\n",
    "# Create list of the crime type labels.  \n",
    "# This will act as the \"labels\" parameter for the log loss functions that follow\n",
    "\n",
    "crime_labels = list(set(y_final))\n",
    "crime_labels_mini_train = list(set(mini_train_labels))\n",
    "crime_labels_mini_dev = list(set(mini_dev_labels))\n",
    "crime_labels_mini_calibrate = list(set(mini_calibrate_labels))\n",
    "\n",
    "print(\"There should be 37 of each type of label -- otherwise, we'll have an issue later:\")\n",
    "print(len(crime_labels), len(crime_labels_mini_train), len(crime_labels_mini_dev),len(crime_labels_mini_calibrate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Formatting Test Data\n",
    "#### The test data needs the same transformations we applied to the training data.\n",
    "\n",
    "*Download the transformed test data from [here](https://drive.google.com/a/berkeley.edu/uc?id=0B74-LZykH7CuYjF5dzN6aHl0YVU&export=download).*\n",
    "\n",
    "*Download the sample submission from [here](https://www.kaggle.com/c/sf-crime/download/sampleSubmission.csv.zip).*\n",
    "\n",
    "To follow along with the notebook, ensure the paths to both in the code below are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Kaggle submission format requires listing the ID of each example.\n",
    "# This is to remember the order of the IDs after shuffling\n",
    "allIDs = np.array(list(df.axes[0]))\n",
    "allIDs = allIDs[shuffle]\n",
    "\n",
    "testIDs = allIDs[800000:]\n",
    "devIDs = allIDs[700000:800000]\n",
    "trainIDs = allIDs[:700000]\n",
    "\n",
    "# Extract the column names for the required submission format\n",
    "sampleSubmission_path = \"/Users/Goodgame/Desktop/prototyping/sampleSubmission.csv\"\n",
    "sampleDF = pd.read_csv(sampleSubmission_path)\n",
    "allColumns = list(sampleDF.columns)\n",
    "featureColumns = allColumns[1:]\n",
    "\n",
    "# Extracting the test data for a baseline submission\n",
    "real_test_path = \"/Users/Goodgame/Desktop/prototyping/test_data_transformed.csv\"\n",
    "testDF = pd.read_csv(real_test_path, header=0)\n",
    "real_test_data = testDF\n",
    "\n",
    "test_complete = real_test_data.fillna(real_test_data.mean())\n",
    "Test_raw = test_complete.as_matrix()\n",
    "\n",
    "test_data = MinMaxScaler().fit_transform(Test_raw)\n",
    "\n",
    "# Remember the ID of each test data point, in case we shuffle the test data\n",
    "testIDs = list(testDF.axes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the code above will shuffle data differently every time it's run, so model accuracies will vary accordingly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Performance Criteria\n",
    "\n",
    "As determined by the Kaggle submission guidelines, the performance criteria metric for the San Francisco Crime Classification competition is Multi-class Logarithmic Loss (also known as cross-entropy).  There are various other performance metrics that are appropriate for different domains: accuracy, F-score, Lift, ROC Area, average precision, precision/recall break-even point, and squared error.\n",
    "\n",
    "Because the Kaggle competition guidelines use multi-class logarithmic loss to score submissions, we will use that metric to gauge the effectiveness of our models in development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Calibration\n",
    "\n",
    "During the course of this project, we tested dozens of models and tens of thousands of model specifications:\n",
    "\n",
    "##### 1) Hyperparameter tuning\n",
    "\n",
    "Each classifier has parameters that we can engineer to further optimize performance, as opposed to using the default parameter values. The approach is specific to each model type.\n",
    "\n",
    "##### 2) Model calibration\n",
    "\n",
    "After tuning hyperparameters, we calibrated the models via Platt Scaling or Isotonic Regression to attempt to improve their performance.\n",
    "\n",
    "We used CalibratedClassifierCV to perform probability calibration with isotonic regression or sigmoid (Platt Scaling).  The parameters within CalibratedClassifierCV allowed us to adjust the method ('sigmoid' or 'isotonic') and cv (cross-validation generator). Because we train our models before calibration, we only use cv = 'prefit'.  Therefore, in practice, the cross-validation generator is not a modifiable parameter for our pipeline.\n",
    "\n",
    "### Models we tuned and calibrated\n",
    "#### To see more about our work with these models, please reference the additional Jupyter notebooks in this repository.\n",
    "1. Multinomial Naive Bayes\n",
    "2. Bernoulli Naive Bayes\n",
    "3. Gaussian Naive Bayes\n",
    "4. Logistic Regression\n",
    "5. Neural Networks (from Theano and MLPClassifier)\n",
    "6. Decision Trees\n",
    "7. K-Nearest Neighbors\n",
    "\n",
    "**Additionally, we examined three types of meta-estimators:**\n",
    "\n",
    "1. AdaBoost Classifier\n",
    "2. Bagging Classifier\n",
    "3. Gradient Boosting Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Winning Model: Random Forest\n",
    "\n",
    "For the Random Forest classifier, we optimized the following classifier parameters: \n",
    "1. n_estimators (the number of trees in the forsest) \n",
    "2. max_features\n",
    "3. max_depth\n",
    "4. min_samples_leaf\n",
    "5. bootstrap (whether or not bootstrap samples are used when building trees)\n",
    "6. oob_score (whether or not out-of-bag samples are used to estimate the generalization accuracy)\n",
    "\n",
    "### Parallelizing GridSearchCV with Spark-sklearn \n",
    "\n",
    "To optimize the parameters, we used GridSearchCV -- with a slight wrinkle. Because we needed GridSearchCV to sort through an incredible number of model specifications with a very large amount of data, we decided to parallelize the process using Spark.\n",
    "\n",
    "Fortunately, there is a PyPI library for doing just that: **spark-sklearn**. Check out the package [here](http://pythonhosted.org/spark-sklearn/).\n",
    "\n",
    "In order to run spark-sklearn, we took the following steps: \n",
    "- Create an AWS EC2 instance (in our case, a c3.8xlarge instance with an Ubuntu Linux operating system, with 32 vCPUs and 60GiB of memory)\n",
    "- Install: Java, Scala, Anaconda, pip, and relevant dependencies (key library: spark_sklearn)\n",
    "- Run GridSearchCV within a SparkContext\n",
    "\n",
    "All of the code is the exact same as a normal GridSearchCV with scikit-learn, except for two lines:\n",
    "\n",
    "$ *from spark_sklearn import GridSearchCV*\n",
    "\n",
    "$ *gs = GridSearchCV(**sc**, clf, param_grid)*\n",
    "\n",
    "\n",
    "In other words, the grid search takes SparkContext as an extra parameter. Because of that, the process can be parallelized across multiple cores, which saves a lot of time.\n",
    "\n",
    "For more information on parallelizing GridSearchCV using Spark, see this DataBricks [tutorial](https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-apache-spark.html) and this [AWS EC2 PySpark tutorial](https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297). *Note: we ran the PySpark code in the PySpark REPL, rather than in a script. We hit issues with dependencies using Python scripts. We appear not to be alone in this issue; [other data scientists](https://twitter.com/sarah_guido/status/672880303891947520?lang=en) have also hit a wall using scikit-learn with Spark.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Accuracy After Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation on test data\n",
    "\n",
    "During development, we were able to achieve a multi-class logarithmic loss of 2.36593209104 using the model specification and calibration below. The isotonic calibration yielded slightly more accuracy than the sigmoid calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_predictions():\n",
    "\n",
    "    random_forest_tuned = RandomForestClassifier(min_impurity_split=1, \n",
    "                                       n_estimators=100, \n",
    "                                       bootstrap= True,\n",
    "                                       max_features=15,\n",
    "                                       criterion='entropy',\n",
    "                                       min_samples_leaf=10,\n",
    "                                       max_depth=None\n",
    "                                      ).fit(all_train_data, all_train_labels)\n",
    "    rf_isotonic = CalibratedClassifierCV(random_forest_tuned, method = 'isotonic', cv = 'prefit')\n",
    "    rf_isotonic.fit(all_train_data, all_train_labels)\n",
    "    return rf_isotonic.predict_proba(test_data)\n",
    "\n",
    "predictions = generate_predictions()\n",
    "resultDF = pd.DataFrame(predictions, columns=featureColumns)\n",
    "\n",
    "# Add the IDs as a final column\n",
    "resultDF.loc[:,'Id'] = pd.Series(testIDs,index=resultDF.index)\n",
    "\n",
    "# Make the 'Id' column the first column, per the requirements\n",
    "colnames = resultDF.columns.tolist()\n",
    "colnames = colnames[-1:] + colnames[:-1]\n",
    "resultDF = resultDF[colnames]\n",
    "\n",
    "# Output to a .csv file\n",
    "resultDF.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis: Calibration (I haven't made this work yet. We actually only submit the prediction probabilities, not the category we predict -- so to get the actual prediction, we'd just have to take the highest probability from the *`predictions`* variable above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_calibration(buckets, clf_probabilities, clf_predictions, labels):\n",
    "    \"\"\"inputs:\n",
    "    clf_probabilities = clf.predict_proba(dev_data)\n",
    "    clf_predictions = clf.predict(dev_data)\n",
    "    labels = dev_labels\"\"\"\n",
    "    \n",
    "    #buckets = [0.05, 0.15, 0.3, 0.5, 0.8]\n",
    "    #buckets = [0.15, 0.25, 0.3, 1.0]\n",
    "    correct = [0 for i in buckets]\n",
    "    total = [0 for i in buckets]\n",
    "\n",
    "    lLimit = 0\n",
    "    uLimit = 0\n",
    "    for i in range(len(buckets)):\n",
    "        uLimit = buckets[i]\n",
    "        for j in range(clf_probabilities.shape[0]):\n",
    "            if (np.amax(clf_probabilities[j]) > lLimit) and (np.amax(clf_probabilities[j]) <= uLimit):\n",
    "                if clf_predictions[j] == labels[j]:\n",
    "                    correct[i] += 1\n",
    "                total[i] += 1\n",
    "        lLimit = uLimit\n",
    "        \n",
    "    print(sum(correct))\n",
    "    print(sum(total))\n",
    "    print(correct)\n",
    "    print(total)\n",
    "\n",
    "    # Report the classifier accuracy for each posterior probability bucket\n",
    "    accuracies = []\n",
    "    for k in range(len(buckets)):\n",
    "        print(1.0*correct[k]/total[k])\n",
    "        accuracies.append(1.0*correct[k]/total[k])\n",
    "        print('p(pred) <= %.13f    total = %3d    correct = %3d    accuracy = %.3f' \\\n",
    "              %(buckets[k], total[k], correct[k], 1.0*correct[k]/total[k]))\n",
    "    plt.plot(buckets,accuracies)\n",
    "    plt.title(\"Calibration Analysis\")\n",
    "    plt.xlabel(\"Posterior Probability\")\n",
    "    plt.ylabel(\"Classifier Accuracy\")\n",
    "    \n",
    "    return buckets, accuracies\n",
    "\n",
    "pd.DataFrame(np.amax(bestLRPredictionProbabilities, axis=1)).hist()\n",
    "\n",
    "buckets = [0.15, 0.25, 0.3, 1.0]\n",
    "calibration_buckets, calibration_accuracies = error_calibration(buckets, clf_probabilities=bestLRPredictionProbabilities, \n",
    "                                                                         clf_predictions=bestLRPredictions,\n",
    "                                                                         labels=mini_dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884262\n",
      "39\n",
      "[  0.000e+00   3.788e-01   0.000e+00   0.000e+00   7.576e-08   0.000e+00\n",
      "   0.000e+00   5.543e-09   7.487e-09   0.000e+00   0.000e+00   0.000e+00\n",
      "   0.000e+00   0.000e+00   0.000e+00   0.000e+00   1.125e-02   0.000e+00\n",
      "   0.000e+00   5.710e-02   1.093e-02   2.268e-02   0.000e+00   0.000e+00\n",
      "   0.000e+00   1.232e-03   0.000e+00   4.726e-08   0.000e+00   0.000e+00\n",
      "   0.000e+00   0.000e+00   2.835e-05   0.000e+00   0.000e+00   4.208e-01\n",
      "   9.715e-02   1.550e-05   0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Each row in 'predictions' is the probability that that feature is the predicted category\n",
    "\n",
    "print(len(predictions))\n",
    "print(len(predictions[0]))\n",
    "print(predictions[0])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis: Classification Report (I haven't made this work yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_report(clf_predictions, labels):\n",
    "    \"\"\"inputs:\n",
    "    clf_predictions = clf.predict(dev_data)\n",
    "    labels = dev_labels\"\"\"\n",
    "    print('Classification Report:')\n",
    "    report = classification_report(labels, clf_predictions)\n",
    "    print(report)\n",
    "    return report\n",
    "\n",
    "\n",
    "report = classification_report(clf_predictions=bestLRPredictions, labels=mini_dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis: Confusion Matrix (I haven't made this work yet)\n",
    "\n",
    "To see the confusion matrix in its entirety, click [here](https://drive.google.com/file/d/0B74-LZykH7CuV0pxZ3VjcktnbWc/view?ts=599bae16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(label_names, clf_predictions, labels):\n",
    "    \"\"\"inputs:\n",
    "    clf_predictions = clf.predict(dev_data)\n",
    "    labels = dev_labels\"\"\"\n",
    "    cm = pd.DataFrame(confusion_matrix(labels, clf_predictions, labels=label_names))\n",
    "    cm.columns=label_names\n",
    "    cm.index=label_names\n",
    "    cm.to_csv(path_or_buf=\"./confusion_matrix.csv\")\n",
    "    #print(cm)\n",
    "    return cm\n",
    "\n",
    "print(confustion_matrix(label_names=crime_labels_mini_dev, clf_predictions=bestLRPredictions,\n",
    "                                                            labels=mini_dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1) Hsiang, Solomon M. and Burke, Marshall and Miguel, Edward. \"Quantifying the Influence of Climate on Human Conflict\". Science, Vol 341, Issue 6151, 2013   \n",
    "\n",
    "2) Huang, Cheng-Lung. Wang, Chieh-Jen. \"A GA-based feature selection and parameters optimization for support vector machines\". Expert Systems with Applications, Vol 31, 2006, p 231-240\n",
    "\n",
    "3) https://gallery.cortanaintelligence.com/Experiment/Evaluating-and-Parameter-Tuning-a-Decision-Tree-Model-1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
