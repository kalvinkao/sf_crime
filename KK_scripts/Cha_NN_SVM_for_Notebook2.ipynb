{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle San Francisco Crime Classification\n",
    "## Berkeley MIDS W207 Final Project: Sam Goodgame, Sarah Cha, Kalvin Kao, Bryan Moore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries:\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import Meta-estimators\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Import Calibration tools\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Set random seed and format print output:\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDL to construct table for SQL transformations:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE kaggle_sf_crime (\n",
    "dates TIMESTAMP,                                \n",
    "category VARCHAR,\n",
    "descript VARCHAR,\n",
    "dayofweek VARCHAR,\n",
    "pd_district VARCHAR,\n",
    "resolution VARCHAR,\n",
    "addr VARCHAR,\n",
    "X FLOAT,\n",
    "Y FLOAT);\n",
    "```\n",
    "#### Getting training data into a locally hosted PostgreSQL database:\n",
    "```sql\n",
    "\\copy kaggle_sf_crime FROM '/Users/Goodgame/Desktop/MIDS/207/final/sf_crime_train.csv' DELIMITER ',' CSV HEADER;\n",
    "```\n",
    "\n",
    "#### SQL Query used for transformations:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  category,\n",
    "  date_part('hour', dates) AS hour_of_day,\n",
    "  CASE\n",
    "    WHEN dayofweek = 'Monday' then 1\n",
    "    WHEN dayofweek = 'Tuesday' THEN 2\n",
    "    WHEN dayofweek = 'Wednesday' THEN 3\n",
    "    WHEN dayofweek = 'Thursday' THEN 4\n",
    "    WHEN dayofweek = 'Friday' THEN 5\n",
    "    WHEN dayofweek = 'Saturday' THEN 6\n",
    "    WHEN dayofweek = 'Sunday' THEN 7\n",
    "  END AS dayofweek_numeric,\n",
    "  X,\n",
    "  Y,\n",
    "  CASE\n",
    "    WHEN pd_district = 'BAYVIEW' THEN 1\n",
    "    ELSE 0\n",
    "  END AS bayview_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'INGLESIDE' THEN 1\n",
    "    ELSE 0\n",
    "  END AS ingleside_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'NORTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS northern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'CENTRAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS central_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'BAYVIEW' THEN 1\n",
    "    ELSE 0\n",
    "  END AS pd_bayview_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'MISSION' THEN 1\n",
    "    ELSE 0\n",
    "  END AS mission_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'SOUTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS southern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TENDERLOIN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS tenderloin_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'PARK' THEN 1\n",
    "    ELSE 0\n",
    "  END AS park_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'RICHMOND' THEN 1\n",
    "    ELSE 0\n",
    "  END AS richmond_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TARAVAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS taraval_binary\n",
    "FROM kaggle_sf_crime;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data, version 2, with weather features to improve performance: (Negated with hashtags for now, as will cause file dependency issues if run locally for everyone. Will be run by Isabell in final notebook with correct files she needs)\n",
    "\n",
    "We seek to add features to our models that will improve performance with respect to out desired performance metric.  There is evidence that there is a correlation between weather patterns and crime, with some experts even arguing for a causal relationship between weather and crime [1].  More specifically, a 2013 paper published in Science showed that higher temperatures and extreme rainfall led to large increases in conflict.  In the setting of strong evidence that weather influences crime, we see it as a candidate for additional features to improve the performance of our classifiers.  Weather data was gathered from (insert source).  Certain features from this data set were incorporated into the original crime data set in order to add features that were hypothesizzed to improve performance.  These features included (insert what we eventually include)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_path = \"./data/train_transformed.csv\"\n",
    "\n",
    "#df = pd.read_csv(data_path, header=0)\n",
    "#x_data = df.drop('category', 1)\n",
    "#y = df.category.as_matrix()\n",
    "\n",
    "########## Adding the date back into the data\n",
    "#import csv\n",
    "#import time\n",
    "#import calendar\n",
    "#data_path = \"./data/train.csv\"\n",
    "#dataCSV = open(data_path, 'rt')\n",
    "#csvData = list(csv.reader(dataCSV))\n",
    "#csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "#allData = csvData[1:]\n",
    "#dataCSV.close()\n",
    "\n",
    "#df2 = pd.DataFrame(allData)\n",
    "#df2.columns = csvFields\n",
    "#dates = df2['Dates']\n",
    "#dates = dates.apply(time.strptime, args=(\"%Y-%m-%d %H:%M:%S\",))\n",
    "#dates = dates.apply(calendar.timegm)\n",
    "#print(dates.head())\n",
    "\n",
    "#x_data['secondsFromEpoch'] = dates\n",
    "#colnames = x_data.columns.tolist()\n",
    "#colnames = colnames[-1:] + colnames[:-1]\n",
    "#x_data = x_data[colnames]\n",
    "##########\n",
    "\n",
    "########## Adding the weather data into the original crime data\n",
    "#weatherData1 = \"./data/1027175.csv\"\n",
    "#weatherData2 = \"./data/1027176.csv\"\n",
    "#dataCSV = open(weatherData1, 'rt')\n",
    "#csvData = list(csv.reader(dataCSV))\n",
    "#csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "#allWeatherData1 = csvData[1:]\n",
    "#dataCSV.close()\n",
    "\n",
    "#dataCSV = open(weatherData2, 'rt')\n",
    "#csvData = list(csv.reader(dataCSV))\n",
    "#csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "#allWeatherData2 = csvData[1:]\n",
    "#dataCSV.close()\n",
    "\n",
    "#weatherDF1 = pd.DataFrame(allWeatherData1)\n",
    "#weatherDF1.columns = csvFields\n",
    "#dates1 = weatherDF1['DATE']\n",
    "#sunrise1 = weatherDF1['DAILYSunrise']\n",
    "#sunset1 = weatherDF1['DAILYSunset']\n",
    "\n",
    "#weatherDF2 = pd.DataFrame(allWeatherData2)\n",
    "#weatherDF2.columns = csvFields\n",
    "#dates2 = weatherDF2['DATE']\n",
    "#sunrise2 = weatherDF2['DAILYSunrise']\n",
    "#sunset2 = weatherDF2['DAILYSunset']\n",
    "\n",
    "#functions for processing the sunrise and sunset times of each day\n",
    "#def get_hour_and_minute(milTime):\n",
    " #   hour = int(milTime[:-2])\n",
    " #   minute = int(milTime[-2:])\n",
    " #   return [hour, minute]\n",
    "\n",
    "#def get_date_only(date):\n",
    "#    return time.struct_time(tuple([date[0], date[1], date[2], 0, 0, 0, date[6], date[7], date[8]]))\n",
    "\n",
    "#def structure_sun_time(timeSeries, dateSeries):\n",
    "#    sunTimes = timeSeries.copy()\n",
    "#    for index in range(len(dateSeries)):\n",
    "#        sunTimes[index] = time.struct_time(tuple([dateSeries[index][0], dateSeries[index][1], dateSeries[index][2], timeSeries[index][0], timeSeries[index][1], dateSeries[index][5], dateSeries[index][6], dateSeries[index][7], dateSeries[index][8]]))\n",
    "#    return sunTimes\n",
    "\n",
    "#dates1 = dates1.apply(time.strptime, args=(\"%Y-%m-%d %H:%M\",))\n",
    "#sunrise1 = sunrise1.apply(get_hour_and_minute)\n",
    "#sunrise1 = structure_sun_time(sunrise1, dates1)\n",
    "#sunrise1 = sunrise1.apply(calendar.timegm)\n",
    "#sunset1 = sunset1.apply(get_hour_and_minute)\n",
    "#sunset1 = structure_sun_time(sunset1, dates1)\n",
    "#sunset1 = sunset1.apply(calendar.timegm)\n",
    "#dates1 = dates1.apply(calendar.timegm)\n",
    "\n",
    "#dates2 = dates2.apply(time.strptime, args=(\"%Y-%m-%d %H:%M\",))\n",
    "#sunrise2 = sunrise2.apply(get_hour_and_minute)\n",
    "#sunrise2 = structure_sun_time(sunrise2, dates2)\n",
    "#sunrise2 = sunrise2.apply(calendar.timegm)\n",
    "#sunset2 = sunset2.apply(get_hour_and_minute)\n",
    "#sunset2 = structure_sun_time(sunset2, dates2)\n",
    "#sunset2 = sunset2.apply(calendar.timegm)\n",
    "#dates2 = dates2.apply(calendar.timegm)\n",
    "\n",
    "#weatherDF1['DATE'] = dates1\n",
    "#weatherDF1['DAILYSunrise'] = sunrise1\n",
    "#weatherDF1['DAILYSunset'] = sunset1\n",
    "#weatherDF2['DATE'] = dates2\n",
    "#weatherDF2['DAILYSunrise'] = sunrise2\n",
    "#weatherDF2['DAILYSunset'] = sunset2\n",
    "\n",
    "#weatherDF = pd.concat([weatherDF1,weatherDF2[32:]],ignore_index=True)\n",
    "\n",
    "# Starting off with some of the easier features to work with-- more to come here . . . still in beta\n",
    "#weatherMetrics = weatherDF[['DATE','HOURLYDRYBULBTEMPF','HOURLYRelativeHumidity', 'HOURLYWindSpeed', \\\n",
    "#                            'HOURLYSeaLevelPressure', 'HOURLYVISIBILITY', 'DAILYSunrise', 'DAILYSunset']]\n",
    "#weatherMetrics = weatherMetrics.convert_objects(convert_numeric=True)\n",
    "#weatherDates = weatherMetrics['DATE']\n",
    "#'DATE','HOURLYDRYBULBTEMPF','HOURLYRelativeHumidity', 'HOURLYWindSpeed',\n",
    "#'HOURLYSeaLevelPressure', 'HOURLYVISIBILITY'\n",
    "#timeWindow = 10800 #3 hours\n",
    "#hourlyDryBulbTemp = []\n",
    "#hourlyRelativeHumidity = []\n",
    "#hourlyWindSpeed = []\n",
    "#hourlySeaLevelPressure = []\n",
    "#hourlyVisibility = []\n",
    "#dailySunrise = []\n",
    "#dailySunset = []\n",
    "#daylight = []\n",
    "#test = 0\n",
    "#for timePoint in dates:#dates is the epoch time from the kaggle data\n",
    "#    relevantWeather = weatherMetrics[(weatherDates <= timePoint) & (weatherDates > timePoint - timeWindow)]\n",
    "#    hourlyDryBulbTemp.append(relevantWeather['HOURLYDRYBULBTEMPF'].mean())\n",
    "#    hourlyRelativeHumidity.append(relevantWeather['HOURLYRelativeHumidity'].mean())\n",
    "#    hourlyWindSpeed.append(relevantWeather['HOURLYWindSpeed'].mean())\n",
    "#    hourlySeaLevelPressure.append(relevantWeather['HOURLYSeaLevelPressure'].mean())\n",
    "#    hourlyVisibility.append(relevantWeather['HOURLYVISIBILITY'].mean())\n",
    "#    dailySunrise.append(relevantWeather['DAILYSunrise'].iloc[-1])\n",
    "#    dailySunset.append(relevantWeather['DAILYSunset'].iloc[-1])\n",
    "#    daylight.append(1.0*((timePoint >= relevantWeather['DAILYSunrise'].iloc[-1]) and (timePoint < relevantWeather['DAILYSunset'].iloc[-1])))\n",
    "    #if timePoint < relevantWeather['DAILYSunset'][-1]:\n",
    "        #daylight.append(1)\n",
    "    #else:\n",
    "        #daylight.append(0)\n",
    "    \n",
    "#    if test%100000 == 0:\n",
    "#        print(relevantWeather)\n",
    "#    test += 1\n",
    "\n",
    "#hourlyDryBulbTemp = pd.Series.from_array(np.array(hourlyDryBulbTemp))\n",
    "#hourlyRelativeHumidity = pd.Series.from_array(np.array(hourlyRelativeHumidity))\n",
    "#hourlyWindSpeed = pd.Series.from_array(np.array(hourlyWindSpeed))\n",
    "#hourlySeaLevelPressure = pd.Series.from_array(np.array(hourlySeaLevelPressure))\n",
    "#hourlyVisibility = pd.Series.from_array(np.array(hourlyVisibility))\n",
    "#dailySunrise = pd.Series.from_array(np.array(dailySunrise))\n",
    "#dailySunset = pd.Series.from_array(np.array(dailySunset))\n",
    "#daylight = pd.Series.from_array(np.array(daylight))\n",
    "\n",
    "#x_data['HOURLYDRYBULBTEMPF'] = hourlyDryBulbTemp\n",
    "#x_data['HOURLYRelativeHumidity'] = hourlyRelativeHumidity\n",
    "#x_data['HOURLYWindSpeed'] = hourlyWindSpeed\n",
    "#x_data['HOURLYSeaLevelPressure'] = hourlySeaLevelPressure\n",
    "#x_data['HOURLYVISIBILITY'] = hourlyVisibility\n",
    "#x_data['DAILYSunrise'] = dailySunrise\n",
    "#x_data['DAILYSunset'] = dailySunset\n",
    "#x_data['Daylight'] = daylight\n",
    "\n",
    "#x_data.to_csv(path_or_buf=\"C:/MIDS/W207 final project/x_data.csv\")\n",
    "##########\n",
    "\n",
    "# Impute missing values with mean values:\n",
    "#x_complete = x_data.fillna(x_data.mean())\n",
    "#X_raw = x_complete.as_matrix()\n",
    "\n",
    "# Scale the data between 0 and 1:\n",
    "#X = MinMaxScaler().fit_transform(X_raw)\n",
    "\n",
    "# Shuffle data to remove any underlying pattern that may exist:\n",
    "#shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "#X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "# Separate training, dev, and test data:\n",
    "#test_data, test_labels = X[800000:], y[800000:]\n",
    "#dev_data, dev_labels = X[700000:800000], y[700000:800000]\n",
    "#train_data, train_labels = X[:700000], y[:700000]\n",
    "\n",
    "#mini_train_data, mini_train_labels = X[:75000], y[:75000]\n",
    "#mini_dev_data, mini_dev_labels = X[75000:100000], y[75000:100000]\n",
    "#labels_set = set(mini_dev_labels)\n",
    "#print(labels_set)\n",
    "#print(len(labels_set))\n",
    "#print(train_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local, individual load of updated data set (with weather data integrated) into training, development, and test subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37 37 37\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/sarahcha/Documents/sf_crime/data/x_data_3.csv\"\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "x_data = df.drop('category', 1)\n",
    "y = df.category.as_matrix()\n",
    "\n",
    "# Impute missing values with mean values:\n",
    "x_complete = x_data.fillna(x_data.mean())\n",
    "X_raw = x_complete.as_matrix()\n",
    "\n",
    "# Scale the data between 0 and 1:\n",
    "X = MinMaxScaler().fit_transform(X_raw)\n",
    "\n",
    "# Shuffle data to remove any underlying pattern that may exist.  Must re-run random seed step each time:\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "# Due to difficulties with log loss and set(y_pred) needing to match set(labels), we will remove the extremely rare\n",
    "# crimes from the data for quality issues.\n",
    "X_minus_trea = X[np.where(y != 'TREA')]\n",
    "y_minus_trea = y[np.where(y != 'TREA')]\n",
    "X_final = X_minus_trea[np.where(y_minus_trea != 'PORNOGRAPHY/OBSCENE MAT')]\n",
    "y_final = y_minus_trea[np.where(y_minus_trea != 'PORNOGRAPHY/OBSCENE MAT')]\n",
    "\n",
    "# Separate training, dev, and test data:\n",
    "test_data, test_labels = X_final[800000:], y_final[800000:]\n",
    "dev_data, dev_labels = X_final[700000:800000], y_final[700000:800000]\n",
    "train_data, train_labels = X_final[100000:700000], y_final[100000:700000]\n",
    "calibrate_data, calibrate_labels = X_final[:100000], y_final[:100000]\n",
    "\n",
    "# Create mini versions of the above sets\n",
    "mini_train_data, mini_train_labels = X_final[:20000], y_final[:20000]\n",
    "mini_calibrate_data, mini_calibrate_labels = X_final[19000:28000], y_final[19000:28000]\n",
    "mini_dev_data, mini_dev_labels = X_final[49000:60000], y_final[49000:60000]\n",
    "\n",
    "# Create list of the crime type labels.  This will act as the \"labels\" parameter for the log loss functions that follow\n",
    "crime_labels = list(set(y_final))\n",
    "crime_labels_mini_train = list(set(mini_train_labels))\n",
    "crime_labels_mini_dev = list(set(mini_dev_labels))\n",
    "crime_labels_mini_calibrate = list(set(mini_calibrate_labels))\n",
    "print(len(crime_labels), len(crime_labels_mini_train), len(crime_labels_mini_dev),len(crime_labels_mini_calibrate))\n",
    "\n",
    "#print(len(train_data),len(train_labels))\n",
    "#print(len(dev_data),len(dev_labels))\n",
    "#print(len(mini_train_data),len(mini_train_labels))\n",
    "#print(len(mini_dev_data),len(mini_dev_labels))\n",
    "#print(len(test_data),len(test_labels))\n",
    "#print(len(mini_calibrate_data),len(mini_calibrate_labels))\n",
    "#print(len(calibrate_data),len(calibrate_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crime_labels_mini_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Nets (Sarah)\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "The benefit of Neural Nets is that it can learn a non-linear function for classification. Relative to that of logistic regression, you have multiple \"hidden\" layers in between the input and output layers. \n",
    "\n",
    "The disadvantages are however that 1) you could have more than one local minimum due to a convex loss function which means depending on where the weights initialize you could end up with very different results and 2) the number of hyperparameters to tune is high -- requires lots of computing power.\n",
    "\n",
    "MLP trains using backpropgation - it trains using gradient descent and for classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates.\n",
    "\n",
    "For the Neural Networks MLP classifier, the following classifier parameters are subject to optimization: 1) hidden_layer_sizes - which includes number of hidden layers as well as number of hidden nodes; 2) activation function - 'identity', 'logistic', 'tanh', 'relu'; 3) alpha - L2 regularization term to avoid overfitting; 4) learning_rate - 'constant', 'invscaling','adaptive'; 5) solver - 'lbfgs','sgd', adam' and 6) batch size. \n",
    "\n",
    "Given the limitations to compute power, we took a targeted approach to training our MLP model. We ran gridsearchCV several times over subsets of parameters which is obviously not ideal given the number of parameters. We also attempted to test the potential impact of scikit's neural network classifier which also can incorporate drop out methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## one of the iterations of gridSearchCV we ran\n",
    "\n",
    "import itertools\n",
    "hidden_layers = [2, 3, 4]\n",
    "for i in hidden_layers:\n",
    "    hidden_layer_sizes = [x for x in itertools.product((4, 8, 12, 16, 20),repeat=i)]  \n",
    "\n",
    "parameters = {'activation' : ('logistic', 'tanh', 'relu'), \n",
    "              'batch_size': (50, 100, 200), \n",
    "              'alpha': (0.001, 0.01, 0.1, 0.25), \n",
    "             'hidden_layer_sizes': hidden_layer_sizes}\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(tol = 0.01), parameters, scoring = \"log_loss\")\n",
    "clf.fit(mini_train_data, mini_train_labels).predict_proba(mini_dev_data)\n",
    "print(\"optimal parameters: \",clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6084294651\n"
     ]
    }
   ],
   "source": [
    "## our best parameter return\n",
    "clf = MLPClassifier(tol=0.01, activation = 'tanh', hidden_layer_sizes = (20,), batch_size = 10, alpha = 0.01).fit(mini_train_data, mini_train_labels)\n",
    "predicted = clf.predict(mini_dev_data)\n",
    "predicted_prob = clf.predict_proba(mini_dev_data)\n",
    "score = log_loss(mini_dev_labels, predicted_prob)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Log Loss with NN is: 2.61062311337\n",
      "Multi-class Log Loss with NN is: 2.71404842724\n",
      "For KNN the best log loss with hyperparameter tuning and calibration is 2.61062311337\n"
     ]
    }
   ],
   "source": [
    "##further calibration using CalibratedClassifierCV\n",
    "list_for_log_loss = []\n",
    "\n",
    "def NN_calibrated():\n",
    "    tuned_NN = clf = MLPClassifier(tol=0.01, activation = 'tanh', hidden_layer_sizes = (20,), batch_size = 10, alpha = 0.001).fit(mini_train_data, mini_train_labels)\n",
    "    dev_prediction_probabilities = tuned_NN.predict_proba(mini_dev_data)\n",
    "    ccv = CalibratedClassifierCV(tuned_NN, method = m, cv = 'prefit')\n",
    "    ccv.fit(mini_calibrate_data, mini_calibrate_labels)\n",
    "    ccv_prediction_probabilities = ccv.predict_proba(mini_dev_data)\n",
    "    working_log_loss = log_loss(y_true = mini_dev_labels, y_pred = ccv_prediction_probabilities, labels = crime_labels_mini_dev)\n",
    "    list_for_log_loss.append(working_log_loss)\n",
    "    print(\"Multi-class Log Loss with NN is:\", working_log_loss)\n",
    "\n",
    "methods = ['sigmoid', 'isotonic']\n",
    "\n",
    "for m in methods:\n",
    "    NN_calibrated()\n",
    "            \n",
    "index_best_logloss = np.argmin(list_for_log_loss)\n",
    "print('For KNN the best log loss with hyperparameter tuning and calibration is',list_for_log_loss[index_best_logloss])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We take the best performing parameters from the tuning procedures above and add dropout method to illustrate the potential impact there. Dropout method would not have materially improved the score had we used it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Layer\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Tanh\", units = 20, dropout = .25),\n",
    "        Layer(\"Softmax\")], batch_size = 10, learning_rate = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11000, 37)]\n",
      "[(11000, 37)]\n",
      "2.6059198823\n"
     ]
    }
   ],
   "source": [
    "clf = nn.fit(mini_train_data, mini_train_labels)\n",
    "predicted = clf.predict(mini_dev_data)\n",
    "predicted_prob = clf.predict_proba(mini_dev_data)\n",
    "score = log_loss(mini_dev_labels, predicted_prob)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6678, 37)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6668, 37)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6654, 37)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6678, 37)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6668, 37)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6654, 37)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6678, 37)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator Classifier modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-e3c06e10b1eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'hidden0__units'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     'hidden1__units': [8, 16, 20]})\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_dev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpredicted_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_dev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \"\"\"\n\u001b[0;32m--> 813\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m                 for train, test in cv)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1611\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# Now train based on a problem transformed into regression.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             log.error(\"\\n{}{}{}\\n\\n{}\\n\".format(\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mis_best_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mavg_train_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mavg_train_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_train_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sknn/backend/lasagne/mlp.py\u001b[0m in \u001b[0;36m_train_impl\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_valid_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/sknn/backend/lasagne/mlp.py\u001b[0m in \u001b[0;36m_batch_impl\u001b[0;34m(self, X, y, w, processor, mode, output, shuffle)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahcha/anaconda3/lib/python3.5/site-packages/theano/tensor/blas.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1547\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1550\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##We attempted to run a more comprehensive hyperparameter tuning procedure including number of iterations and ranges for dropout method. It was too computationally intensive. Even on a remote AWS instance on pyspark it kept stalling out. \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(nn, param_grid={\n",
    "    'learning_rate': [0.05, 0.01, 0.005, 0.001],\n",
    "    'n_iter': [25, 50, 100, 200],\n",
    "    'hidden0__units': [4, 8, 12, 16, 20],\n",
    "    'hidden0__type': [\"Rectifier\", \"Sigmoid\", \"Tanh\"],\n",
    "    'hidden0__dropout':[0.2, 0.3, 0.4],\n",
    "    'hidden1__units': [4, 8, 12, 16, 20],\n",
    "    'hidden1__type': [\"Rectifier\", \"Sigmoid\", \"Tanh\"],\n",
    "    'hidden1__dropout':[0.2, 0.3, 0.4],\n",
    "    'hidden2__units': [4, 8, 12, 16, 20],\n",
    "    'hidden2__type': [\"Rectifier\", \"Sigmoid\", \"Tanh\"],\n",
    "    'hidden2__dropout':[0.2, 0.3, 0.4]})\n",
    "gs.fit(mini_train_data, mini_train_labels)\n",
    "predicted = gs.predict(mini_dev_data)\n",
    "predicted_prob = gs.predict_proba(mini_dev_data)\n",
    "score = log_loss(mini_dev_labels, predicted_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (Sarah)\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the SVM classifier, we can seek to optimize the following classifier parameters: C (penalty parameter C of the error term), kernel ('linear', 'poly', 'rbf', sigmoid', or 'precomputed')\n",
    "\n",
    "See source [2] for parameter optimization in SVM\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "class_labels = list(set(train_labels))\n",
    "\n",
    "def enumerateY(data):\n",
    "    enumerated_data = np.zeros(data.size)\n",
    "    for j in range(0,data.size):\n",
    "        feature = data[j]\n",
    "        i = class_labels.index(feature)\n",
    "        enumerated_data[j]+= i\n",
    "    return (np.array(enumerated_data))\n",
    "\n",
    "\n",
    "train_labels_enum = enumerateY(mini_train_labels)\n",
    "dev_labels_enum = enumerateY(mini_dev_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##gridSearchCV to find optimal parameters\n",
    "\n",
    "parameters = {'C': (0.001, 0.01, 0.1, 0.5, 1, 5, 10, 25, 100), \n",
    "             'decision_function_shape': ('ovo', 'ovr'),\n",
    "             'kernel': ('linear', 'sigmoid', 'rbf', 'poly')}\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(tol = 0.01, probability = True), parameters, scoring = \"log_loss\")\n",
    "clf.fit(mini_train_data, mini_train_labels).predict_proba(mini_dev_data)\n",
    "print(\"optimal parameters:\", clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##most optimal parameters returned score of 2.62651014795\n",
    "clf_svc = svm.SVC(kernel='rbf', C=100, decision_function_shape='ovr', probability = True).fit(mini_train_data, train_labels_enum)\n",
    "predicted = clf_svc.fit(mini_train_data, train_labels_enum).predict_proba(mini_dev_data)\n",
    "\n",
    "\n",
    "print(log_loss(y_true = dev_labels_enum, y_pred = predicted, labels = dev_labels_enum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (Sam, possibly in AWS)\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Random Forest classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_features, max_depth, min_samples_leaf, bootstrap (whether or not bootstrap samples are used when building trees), oob_score (whether or not out-of-bag samples are used to estimate the generalization accuracy)\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "There are no major changes that we seek to make in the AdaBoostClassifier with respect to default parameter values.\n",
    "\n",
    "###### Adaboosting each classifier:\n",
    "\n",
    "We will run the AdaBoostClassifier on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Bagging meta classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_samples, max_features, bootstrap (whether or not bootstrap samples are used when building trees), bootstrap_features (whether features are drawn with replacement), and oob_score (whether or not out-of-bag samples are used to estimate the generalization accuracy)\n",
    "\n",
    "###### Bagging each classifier:\n",
    "\n",
    "We will run the BaggingClassifier on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Gradient Boosting meta classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_depth, min_samples_leaf, and max_features\n",
    "\n",
    "###### Gradient Boosting each classifier:\n",
    "\n",
    "We will run the GradientBoostingClassifier with loss = 'deviance' (as loss = 'exponential' uses the AdaBoost algorithm) on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we will likely use Pipeline and GridSearchCV in order to find the overall classifier with optimized Multi-class Log Loss.\n",
    "# This will be the last step after all attempts at feature addition, hyperparameter tuning, and calibration are completed\n",
    "# and the corresponding performance metrics are gathered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1) Hsiang, Solomon M. and Burke, Marshall and Miguel, Edward. \"Quantifying the Influence of Climate on Human Conflict\". Science, Vol 341, Issue 6151, 2013   \n",
    "\n",
    "2) Huang, Cheng-Lung. Wang, Chieh-Jen. \"A GA-based feature selection and parameters optimization for support vector machines\". Expert Systems with Applications, Vol 31, 2006, p 231-240\n",
    "\n",
    "3) More to come \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
