{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle San Francisco Crime Classification\n",
    "## Berkeley MIDS W207 Final Project: Sam Goodgame, Sarah Cha, Kalvin Kao, Bryan Moore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries:\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import Meta-estimators\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Import Calibration tools\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Set random seed and format print output:\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDL to construct table for SQL transformations:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE kaggle_sf_crime (\n",
    "dates TIMESTAMP,                                \n",
    "category VARCHAR,\n",
    "descript VARCHAR,\n",
    "dayofweek VARCHAR,\n",
    "pd_district VARCHAR,\n",
    "resolution VARCHAR,\n",
    "addr VARCHAR,\n",
    "X FLOAT,\n",
    "Y FLOAT);\n",
    "```\n",
    "#### Getting training data into a locally hosted PostgreSQL database:\n",
    "```sql\n",
    "\\copy kaggle_sf_crime FROM '/Users/Goodgame/Desktop/MIDS/207/final/sf_crime_train.csv' DELIMITER ',' CSV HEADER;\n",
    "```\n",
    "\n",
    "#### SQL Query used for transformations:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  category,\n",
    "  date_part('hour', dates) AS hour_of_day,\n",
    "  CASE\n",
    "    WHEN dayofweek = 'Monday' then 1\n",
    "    WHEN dayofweek = 'Tuesday' THEN 2\n",
    "    WHEN dayofweek = 'Wednesday' THEN 3\n",
    "    WHEN dayofweek = 'Thursday' THEN 4\n",
    "    WHEN dayofweek = 'Friday' THEN 5\n",
    "    WHEN dayofweek = 'Saturday' THEN 6\n",
    "    WHEN dayofweek = 'Sunday' THEN 7\n",
    "  END AS dayofweek_numeric,\n",
    "  X,\n",
    "  Y,\n",
    "  CASE\n",
    "    WHEN pd_district = 'BAYVIEW' THEN 1\n",
    "    ELSE 0\n",
    "  END AS bayview_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'INGLESIDE' THEN 1\n",
    "    ELSE 0\n",
    "  END AS ingleside_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'NORTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS northern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'CENTRAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS central_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'BAYVIEW' THEN 1\n",
    "    ELSE 0\n",
    "  END AS pd_bayview_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'MISSION' THEN 1\n",
    "    ELSE 0\n",
    "  END AS mission_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'SOUTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS southern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TENDERLOIN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS tenderloin_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'PARK' THEN 1\n",
    "    ELSE 0\n",
    "  END AS park_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'RICHMOND' THEN 1\n",
    "    ELSE 0\n",
    "  END AS richmond_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TARAVAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS taraval_binary\n",
    "FROM kaggle_sf_crime;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data, version 2, with weather features to improve performance: (Negated with hashtags for now, as will cause file dependency issues if run locally for everyone. Will be run by Isabell in final notebook with correct files she needs)\n",
    "\n",
    "We seek to add features to our models that will improve performance with respect to out desired performance metric.  There is evidence that there is a correlation between weather patterns and crime, with some experts even arguing for a causal relationship between weather and crime [1].  More specifically, a 2013 paper published in Science showed that higher temperatures and extreme rainfall led to large increases in conflict.  In the setting of strong evidence that weather influences crime, we see it as a candidate for additional features to improve the performance of our classifiers.  Weather data was gathered from (insert source).  Certain features from this data set were incorporated into the original crime data set in order to add features that were hypothesizzed to improve performance.  These features included (insert what we eventually include)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_path = \"./data/train_transformed.csv\"\n",
    "\n",
    "#df = pd.read_csv(data_path, header=0)\n",
    "#x_data = df.drop('category', 1)\n",
    "#y = df.category.as_matrix()\n",
    "\n",
    "########## Adding the date back into the data\n",
    "#import csv\n",
    "#import time\n",
    "#import calendar\n",
    "#data_path = \"./data/train.csv\"\n",
    "#dataCSV = open(data_path, 'rt')\n",
    "#csvData = list(csv.reader(dataCSV))\n",
    "#csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "#allData = csvData[1:]\n",
    "#dataCSV.close()\n",
    "\n",
    "#df2 = pd.DataFrame(allData)\n",
    "#df2.columns = csvFields\n",
    "#dates = df2['Dates']\n",
    "#dates = dates.apply(time.strptime, args=(\"%Y-%m-%d %H:%M:%S\",))\n",
    "#dates = dates.apply(calendar.timegm)\n",
    "#print(dates.head())\n",
    "\n",
    "#x_data['secondsFromEpoch'] = dates\n",
    "#colnames = x_data.columns.tolist()\n",
    "#colnames = colnames[-1:] + colnames[:-1]\n",
    "#x_data = x_data[colnames]\n",
    "##########\n",
    "\n",
    "########## Adding the weather data into the original crime data\n",
    "#weatherData1 = \"./data/1027175.csv\"\n",
    "#weatherData2 = \"./data/1027176.csv\"\n",
    "#dataCSV = open(weatherData1, 'rt')\n",
    "#csvData = list(csv.reader(dataCSV))\n",
    "#csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "#allWeatherData1 = csvData[1:]\n",
    "#dataCSV.close()\n",
    "\n",
    "#dataCSV = open(weatherData2, 'rt')\n",
    "#csvData = list(csv.reader(dataCSV))\n",
    "#csvFields = csvData[0] #['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "#allWeatherData2 = csvData[1:]\n",
    "#dataCSV.close()\n",
    "\n",
    "#weatherDF1 = pd.DataFrame(allWeatherData1)\n",
    "#weatherDF1.columns = csvFields\n",
    "#dates1 = weatherDF1['DATE']\n",
    "#sunrise1 = weatherDF1['DAILYSunrise']\n",
    "#sunset1 = weatherDF1['DAILYSunset']\n",
    "\n",
    "#weatherDF2 = pd.DataFrame(allWeatherData2)\n",
    "#weatherDF2.columns = csvFields\n",
    "#dates2 = weatherDF2['DATE']\n",
    "#sunrise2 = weatherDF2['DAILYSunrise']\n",
    "#sunset2 = weatherDF2['DAILYSunset']\n",
    "\n",
    "#functions for processing the sunrise and sunset times of each day\n",
    "#def get_hour_and_minute(milTime):\n",
    " #   hour = int(milTime[:-2])\n",
    " #   minute = int(milTime[-2:])\n",
    " #   return [hour, minute]\n",
    "\n",
    "#def get_date_only(date):\n",
    "#    return time.struct_time(tuple([date[0], date[1], date[2], 0, 0, 0, date[6], date[7], date[8]]))\n",
    "\n",
    "#def structure_sun_time(timeSeries, dateSeries):\n",
    "#    sunTimes = timeSeries.copy()\n",
    "#    for index in range(len(dateSeries)):\n",
    "#        sunTimes[index] = time.struct_time(tuple([dateSeries[index][0], dateSeries[index][1], dateSeries[index][2], timeSeries[index][0], timeSeries[index][1], dateSeries[index][5], dateSeries[index][6], dateSeries[index][7], dateSeries[index][8]]))\n",
    "#    return sunTimes\n",
    "\n",
    "#dates1 = dates1.apply(time.strptime, args=(\"%Y-%m-%d %H:%M\",))\n",
    "#sunrise1 = sunrise1.apply(get_hour_and_minute)\n",
    "#sunrise1 = structure_sun_time(sunrise1, dates1)\n",
    "#sunrise1 = sunrise1.apply(calendar.timegm)\n",
    "#sunset1 = sunset1.apply(get_hour_and_minute)\n",
    "#sunset1 = structure_sun_time(sunset1, dates1)\n",
    "#sunset1 = sunset1.apply(calendar.timegm)\n",
    "#dates1 = dates1.apply(calendar.timegm)\n",
    "\n",
    "#dates2 = dates2.apply(time.strptime, args=(\"%Y-%m-%d %H:%M\",))\n",
    "#sunrise2 = sunrise2.apply(get_hour_and_minute)\n",
    "#sunrise2 = structure_sun_time(sunrise2, dates2)\n",
    "#sunrise2 = sunrise2.apply(calendar.timegm)\n",
    "#sunset2 = sunset2.apply(get_hour_and_minute)\n",
    "#sunset2 = structure_sun_time(sunset2, dates2)\n",
    "#sunset2 = sunset2.apply(calendar.timegm)\n",
    "#dates2 = dates2.apply(calendar.timegm)\n",
    "\n",
    "#weatherDF1['DATE'] = dates1\n",
    "#weatherDF1['DAILYSunrise'] = sunrise1\n",
    "#weatherDF1['DAILYSunset'] = sunset1\n",
    "#weatherDF2['DATE'] = dates2\n",
    "#weatherDF2['DAILYSunrise'] = sunrise2\n",
    "#weatherDF2['DAILYSunset'] = sunset2\n",
    "\n",
    "#weatherDF = pd.concat([weatherDF1,weatherDF2[32:]],ignore_index=True)\n",
    "\n",
    "# Starting off with some of the easier features to work with-- more to come here . . . still in beta\n",
    "#weatherMetrics = weatherDF[['DATE','HOURLYDRYBULBTEMPF','HOURLYRelativeHumidity', 'HOURLYWindSpeed', \\\n",
    "#                            'HOURLYSeaLevelPressure', 'HOURLYVISIBILITY', 'DAILYSunrise', 'DAILYSunset']]\n",
    "#weatherMetrics = weatherMetrics.convert_objects(convert_numeric=True)\n",
    "#weatherDates = weatherMetrics['DATE']\n",
    "#'DATE','HOURLYDRYBULBTEMPF','HOURLYRelativeHumidity', 'HOURLYWindSpeed',\n",
    "#'HOURLYSeaLevelPressure', 'HOURLYVISIBILITY'\n",
    "#timeWindow = 10800 #3 hours\n",
    "#hourlyDryBulbTemp = []\n",
    "#hourlyRelativeHumidity = []\n",
    "#hourlyWindSpeed = []\n",
    "#hourlySeaLevelPressure = []\n",
    "#hourlyVisibility = []\n",
    "#dailySunrise = []\n",
    "#dailySunset = []\n",
    "#daylight = []\n",
    "#test = 0\n",
    "#for timePoint in dates:#dates is the epoch time from the kaggle data\n",
    "#    relevantWeather = weatherMetrics[(weatherDates <= timePoint) & (weatherDates > timePoint - timeWindow)]\n",
    "#    hourlyDryBulbTemp.append(relevantWeather['HOURLYDRYBULBTEMPF'].mean())\n",
    "#    hourlyRelativeHumidity.append(relevantWeather['HOURLYRelativeHumidity'].mean())\n",
    "#    hourlyWindSpeed.append(relevantWeather['HOURLYWindSpeed'].mean())\n",
    "#    hourlySeaLevelPressure.append(relevantWeather['HOURLYSeaLevelPressure'].mean())\n",
    "#    hourlyVisibility.append(relevantWeather['HOURLYVISIBILITY'].mean())\n",
    "#    dailySunrise.append(relevantWeather['DAILYSunrise'].iloc[-1])\n",
    "#    dailySunset.append(relevantWeather['DAILYSunset'].iloc[-1])\n",
    "#    daylight.append(1.0*((timePoint >= relevantWeather['DAILYSunrise'].iloc[-1]) and (timePoint < relevantWeather['DAILYSunset'].iloc[-1])))\n",
    "    #if timePoint < relevantWeather['DAILYSunset'][-1]:\n",
    "        #daylight.append(1)\n",
    "    #else:\n",
    "        #daylight.append(0)\n",
    "    \n",
    "#    if test%100000 == 0:\n",
    "#        print(relevantWeather)\n",
    "#    test += 1\n",
    "\n",
    "#hourlyDryBulbTemp = pd.Series.from_array(np.array(hourlyDryBulbTemp))\n",
    "#hourlyRelativeHumidity = pd.Series.from_array(np.array(hourlyRelativeHumidity))\n",
    "#hourlyWindSpeed = pd.Series.from_array(np.array(hourlyWindSpeed))\n",
    "#hourlySeaLevelPressure = pd.Series.from_array(np.array(hourlySeaLevelPressure))\n",
    "#hourlyVisibility = pd.Series.from_array(np.array(hourlyVisibility))\n",
    "#dailySunrise = pd.Series.from_array(np.array(dailySunrise))\n",
    "#dailySunset = pd.Series.from_array(np.array(dailySunset))\n",
    "#daylight = pd.Series.from_array(np.array(daylight))\n",
    "\n",
    "#x_data['HOURLYDRYBULBTEMPF'] = hourlyDryBulbTemp\n",
    "#x_data['HOURLYRelativeHumidity'] = hourlyRelativeHumidity\n",
    "#x_data['HOURLYWindSpeed'] = hourlyWindSpeed\n",
    "#x_data['HOURLYSeaLevelPressure'] = hourlySeaLevelPressure\n",
    "#x_data['HOURLYVISIBILITY'] = hourlyVisibility\n",
    "#x_data['DAILYSunrise'] = dailySunrise\n",
    "#x_data['DAILYSunset'] = dailySunset\n",
    "#x_data['Daylight'] = daylight\n",
    "\n",
    "#x_data.to_csv(path_or_buf=\"C:/MIDS/W207 final project/x_data.csv\")\n",
    "##########\n",
    "\n",
    "# Impute missing values with mean values:\n",
    "#x_complete = x_data.fillna(x_data.mean())\n",
    "#X_raw = x_complete.as_matrix()\n",
    "\n",
    "# Scale the data between 0 and 1:\n",
    "#X = MinMaxScaler().fit_transform(X_raw)\n",
    "\n",
    "# Shuffle data to remove any underlying pattern that may exist:\n",
    "#shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "#X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "# Separate training, dev, and test data:\n",
    "#test_data, test_labels = X[800000:], y[800000:]\n",
    "#dev_data, dev_labels = X[700000:800000], y[700000:800000]\n",
    "#train_data, train_labels = X[:700000], y[:700000]\n",
    "\n",
    "#mini_train_data, mini_train_labels = X[:75000], y[:75000]\n",
    "#mini_dev_data, mini_dev_labels = X[75000:100000], y[75000:100000]\n",
    "#labels_set = set(mini_dev_labels)\n",
    "#print(labels_set)\n",
    "#print(len(labels_set))\n",
    "#print(train_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local, individual load of updated data set (with weather data integrated) into training, development, and test subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37 37 37\n"
     ]
    }
   ],
   "source": [
    "# Data path to your local copy of Sam's \"train_transformed.csv\", which was produced by ?separate Python script?\n",
    "data_path_for_labels_only = \"/Users/Bryan/Desktop/UC_Berkeley_MIDS_files/Courses/W207_Intro_To_Machine_Learning/Final_Project/sf_crime-master/data/train_transformed.csv\"\n",
    "df = pd.read_csv(data_path_for_labels_only, header=0)\n",
    "y = df.category.as_matrix()\n",
    "\n",
    "# Data path to your local copy of Kalvin's \"x_data.csv\", which was produced by the negated cell above\n",
    "data_path = \"/Users/Bryan/Desktop/UC_Berkeley_MIDS_files/Courses/W207_Intro_To_Machine_Learning/Final_Project/x_data_08_15.csv\"\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "\n",
    "# Impute missing values with mean values:\n",
    "x_complete = df.fillna(df.mean())\n",
    "X_raw = x_complete.as_matrix()\n",
    "\n",
    "# Scale the data between 0 and 1:\n",
    "X = MinMaxScaler().fit_transform(X_raw)\n",
    "\n",
    "# Shuffle data to remove any underlying pattern that may exist.  Must re-run random seed step each time:\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "# Due to difficulties with log loss and set(y_pred) needing to match set(labels), we will remove the extremely rare\n",
    "# crimes from the data for quality issues.\n",
    "X_minus_trea = X[np.where(y != 'TREA')]\n",
    "y_minus_trea = y[np.where(y != 'TREA')]\n",
    "X_final = X_minus_trea[np.where(y_minus_trea != 'PORNOGRAPHY/OBSCENE MAT')]\n",
    "y_final = y_minus_trea[np.where(y_minus_trea != 'PORNOGRAPHY/OBSCENE MAT')]\n",
    "\n",
    "# Separate training, dev, and test data:\n",
    "test_data, test_labels = X_final[800000:], y_final[800000:]\n",
    "dev_data, dev_labels = X_final[700000:800000], y_final[700000:800000]\n",
    "train_data, train_labels = X_final[100000:700000], y_final[100000:700000]\n",
    "calibrate_data, calibrate_labels = X_final[:100000], y_final[:100000]\n",
    "\n",
    "# Create mini versions of the above sets\n",
    "mini_train_data, mini_train_labels = X_final[:20000], y_final[:20000]\n",
    "mini_calibrate_data, mini_calibrate_labels = X_final[19000:28000], y_final[19000:28000]\n",
    "mini_dev_data, mini_dev_labels = X_final[49000:60000], y_final[49000:60000]\n",
    "\n",
    "# Create list of the crime type labels.  This will act as the \"labels\" parameter for the log loss functions that follow\n",
    "crime_labels = list(set(y_final))\n",
    "crime_labels_mini_train = list(set(mini_train_labels))\n",
    "crime_labels_mini_dev = list(set(mini_dev_labels))\n",
    "crime_labels_mini_calibrate = list(set(mini_calibrate_labels))\n",
    "print(len(crime_labels), len(crime_labels_mini_train), len(crime_labels_mini_dev),len(crime_labels_mini_calibrate))\n",
    "\n",
    "#print(len(train_data),len(train_labels))\n",
    "#print(len(dev_data),len(dev_labels))\n",
    "#print(len(mini_train_data),len(mini_train_labels))\n",
    "#print(len(mini_dev_data),len(mini_dev_labels))\n",
    "#print(len(test_data),len(test_labels))\n",
    "#print(len(mini_calibrate_data),len(mini_calibrate_labels))\n",
    "#print(len(calibrate_data),len(calibrate_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarah's School data that we may still get to work as features: (Negated with hashtags for now, as will cause file dependency issues if run locally for everyone. Will be run by Isabell in final notebook with correct files she needs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Read in zip code data\n",
    "#data_path_zip = \"./data/2016_zips.csv\"\n",
    "#zips = pd.read_csv(data_path_zip, header=0, sep ='\\t', usecols = [0,5,6], names = [\"GEOID\", \"INTPTLAT\", \"INTPTLONG\"], dtype ={'GEOID': int, 'INTPTLAT': float, 'INTPTLONG': float})\n",
    "#sf_zips = zips[(zips['GEOID'] > 94000) & (zips['GEOID'] < 94189)]\n",
    "\n",
    "### Mapping longitude/latitude to zipcodes\n",
    "#def dist(lat1, long1, lat2, long2):\n",
    "#    return np.sqrt((lat1-lat2)**2+(long1-long2)**2)\n",
    "#    return abs(lat1-lat2)+abs(long1-long2)\n",
    "#def find_zipcode(lat, long):    \n",
    "#    distances = sf_zips.apply(lambda row: dist(lat, long, row[\"INTPTLAT\"], row[\"INTPTLONG\"]), axis=1)\n",
    "#    return sf_zips.loc[distances.idxmin(), \"GEOID\"]\n",
    "#x_data['zipcode'] = 0\n",
    "#for i in range(0, 1):\n",
    "#    x_data['zipcode'][i] = x_data.apply(lambda row: find_zipcode(row['x'], row['y']), axis=1)\n",
    "#x_data['zipcode']= x_data.apply(lambda row: find_zipcode(row['x'], row['y']), axis=1)\n",
    "\n",
    "\n",
    "### Read in school data\n",
    "#data_path_schools = \"./data/pubschls.csv\"\n",
    "#schools = pd.read_csv(data_path_schools,header=0, sep ='\\t', usecols = [\"CDSCode\",\"StatusType\", \"School\", \"EILCode\", \"EILName\", \"Zip\", \"Latitude\", \"Longitude\"], dtype ={'CDSCode': str, 'StatusType': str, 'School': str, 'EILCode': str,'EILName': str,'Zip': str, 'Latitude': float, 'Longitude': float})\n",
    "#schools = schools[(schools[\"StatusType\"] == 'Active')]\n",
    "\n",
    "### Find the closest school\n",
    "#def dist(lat1, long1, lat2, long2):\n",
    "#    return np.sqrt((lat1-lat2)**2+(long1-long2)**2)\n",
    "\n",
    "#def find_closest_school(lat, long):    \n",
    "#    distances = schools.apply(lambda row: dist(lat, long, row[\"Latitude\"], row[\"Longitude\"]), axis=1)\n",
    "#    return min(distances)\n",
    "#x_data['closest_school'] = x_data_sub.apply(lambda row: find_closest_school(row['y'], row['x']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting to meet Kaggle submission standards: (Negated with hashtags for now, as will cause file dependency issues if run locally for everyone. Will be run by Isabell in final notebook with correct files she needs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Kaggle submission format requires listing the ID of each example.\n",
    "# This is to remember the order of the IDs after shuffling\n",
    "#allIDs = np.array(list(df.axes[0]))\n",
    "#allIDs = allIDs[shuffle]\n",
    "\n",
    "#testIDs = allIDs[800000:]\n",
    "#devIDs = allIDs[700000:800000]\n",
    "#trainIDs = allIDs[:700000]\n",
    "\n",
    "# Extract the column names for the required submission format\n",
    "#sampleSubmission_path = \"./data/sampleSubmission.csv\"\n",
    "#sampleDF = pd.read_csv(sampleSubmission_path)\n",
    "#allColumns = list(sampleDF.columns)\n",
    "#featureColumns = allColumns[1:]\n",
    "\n",
    "# Extracting the test data for a baseline submission\n",
    "#real_test_path = \"./data/test_transformed.csv\"\n",
    "#testDF = pd.read_csv(real_test_path, header=0)\n",
    "#real_test_data = testDF\n",
    "\n",
    "#test_complete = real_test_data.fillna(real_test_data.mean())\n",
    "#Test_raw = test_complete.as_matrix()\n",
    "\n",
    "#TestData = MinMaxScaler().fit_transform(Test_raw)\n",
    "\n",
    "# Here we remember the ID of each test data point, in case we ever decide to shuffle the test data for some reason\n",
    "#testIDs = list(testDF.axes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate baseline prediction probabilities from MNB classifier and store in a .csv file (Negated with hashtags for now, as will cause file dependency issues if run locally for everyone. Will be run by Isabell in final notebook with correct files she needs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a baseline MNB classifier and make it return prediction probabilities for the actual test data\n",
    "#def MNB():\n",
    "#    mnb = MultinomialNB(alpha = 0.0000001)\n",
    "#    mnb.fit(train_data, train_labels)\n",
    "#    print(\"\\n\\nMultinomialNB accuracy on dev data:\", mnb.score(dev_data, dev_labels))\n",
    "#    return mnb.predict_proba(dev_data)\n",
    "#MNB()\n",
    "\n",
    "#baselinePredictionProbabilities = MNB()\n",
    "\n",
    "# Place the resulting prediction probabilities in a .csv file in the required format\n",
    "# First, turn the prediction probabilties into a data frame\n",
    "#resultDF = pd.DataFrame(baselinePredictionProbabilities,columns=featureColumns)\n",
    "# Add the IDs as a final column\n",
    "#resultDF.loc[:,'Id'] = pd.Series(testIDs,index=resultDF.index)\n",
    "# Make the 'Id' column the first column\n",
    "#colnames = resultDF.columns.tolist()\n",
    "#colnames = colnames[-1:] + colnames[:-1]\n",
    "#resultDF = resultDF[colnames]\n",
    "# Output to a .csv file\n",
    "# resultDF.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the code above will shuffle data differently every time it's run, so model accuracies will vary accordingly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.016  0.985  0.826  0.667  0.055  0.002  0.     0.     0.     1.     0.\n",
      "   0.     0.     0.     0.     0.     0.     0.514  0.405  0.375  0.661  1.\n",
      "   0.985  0.985  0.   ]]\n",
      "['LARCENY/THEFT']\n"
     ]
    }
   ],
   "source": [
    "## Data sub-setting quality check-point\n",
    "print(train_data[:1])\n",
    "print(train_labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MultinomialNB accuracy on dev data: 0.22347\n"
     ]
    }
   ],
   "source": [
    "# Modeling quality check-point with MNB--fast model\n",
    "\n",
    "def MNB():\n",
    "    mnb = MultinomialNB(alpha = 0.0000001)\n",
    "    mnb.fit(train_data, train_labels)\n",
    "    print(\"\\n\\nMultinomialNB accuracy on dev data:\", mnb.score(dev_data, dev_labels))\n",
    "    \n",
    "MNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Performance Criteria\n",
    "\n",
    "As determined by the Kaggle submission guidelines, the performance criteria metric for the San Francisco Crime Classification competition is Multi-class Logarithmic Loss (also known as cross-entropy).  There are various other performance metrics that are appropriate for different domains: accuracy, F-score, Lift, ROC Area, average precision, precision/recall break-even point, and squared error.\n",
    "\n",
    "(Describe each performance metric and a domain in which it is preferred. Give Pros/Cons if able)\n",
    "\n",
    "- Multi-class Log Loss:\n",
    "\n",
    "- Accuracy:\n",
    "\n",
    "- F-score:\n",
    "\n",
    "- Lift:\n",
    "\n",
    "- ROC Area:\n",
    "\n",
    "- Average precision\n",
    "\n",
    "- Precision/Recall break-even point:\n",
    "\n",
    "- Squared-error:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prototyping\n",
    "We will start our classifier and feature engineering process by looking at the performance of various classifiers with default parameter settings in predicting labels on the mini_dev_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') Multi-class Log Loss: 21.0240643644 \n",
      "\n",
      "\n",
      "BernoulliNB(alpha=1, binarize=0.5, class_prior=None, fit_prior=True) Multi-class Log Loss: 2.6947927812 \n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) Multi-class Log Loss: 2.60974496429 \n",
      "\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) Multi-class Log Loss: 2.59547592791 \n",
      "\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False) Multi-class Log Loss: 2.59629075567 \n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False) Multi-class Log Loss: 15.4363991752 \n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best') Multi-class Log Loss: 29.8258033614 \n",
      "\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False) Multi-class Log Loss: 2.62455308617 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_prototype(train_data, train_labels, eval_data, eval_labels):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5).fit(train_data, train_labels)\n",
    "    bnb = BernoulliNB(alpha=1, binarize = 0.5).fit(train_data, train_labels)\n",
    "    mnb = MultinomialNB().fit(train_data, train_labels)\n",
    "    log_reg = LogisticRegression().fit(train_data, train_labels)\n",
    "    neural_net = MLPClassifier().fit(train_data, train_labels)\n",
    "    random_forest = RandomForestClassifier().fit(train_data, train_labels)\n",
    "    decision_tree = DecisionTreeClassifier().fit(train_data, train_labels)\n",
    "    support_vm_step_one = svm.SVC(probability = True)\n",
    "    support_vm = support_vm_step_one.fit(train_data, train_labels)\n",
    "    \n",
    "    models = [knn, bnb, mnb, log_reg, neural_net, random_forest, decision_tree, support_vm]\n",
    "    for model in models:\n",
    "        eval_prediction_probabilities = model.predict_proba(eval_data)\n",
    "        eval_predictions = model.predict(eval_data)\n",
    "        print(model, \"Multi-class Log Loss:\", log_loss(y_true = eval_labels, y_pred = eval_prediction_probabilities, labels = crime_labels_mini_dev), \"\\n\\n\")\n",
    "\n",
    "model_prototype(mini_train_data, mini_train_labels, mini_dev_data, mini_dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features, Hyperparameter Tuning, and Model Calibration To Improve Prediction For Each Classifier\n",
    "\n",
    "Here we seek to optimize the performance of our classifiers in a three-step, dynamnic engineering process. \n",
    "\n",
    "##### 1) Feature addition\n",
    "\n",
    "We previously added components from the weather data into the original SF crime data as new features.  We will not repeat work done in our initial submission, where our training dataset did not include these features.  For comparision with respoect to how the added features improved our performance with respect to log loss, please refer back to our initial submission.\n",
    "\n",
    "We can have Kalvin expand on exactly what he did here.\n",
    "\n",
    "##### 2) Hyperparameter tuning\n",
    "\n",
    "Each classifier has parameters that we can engineer to further optimize performance, as opposed to using the default parameter values as we did above in the model prototyping cell. This will be specific to each classifier as detailed below.\n",
    "\n",
    "##### 3) Model calibration\n",
    "\n",
    "We can calibrate the models via Platt Scaling or Isotonic Regression to attempt to improve their performance.\n",
    "\n",
    "- Platt Scaling: ((brief explanation of how it works))\n",
    "\n",
    "- Isotonic Regression: ((brief explanation of how it works))\n",
    "\n",
    "For each classifier, we can use CalibratedClassifierCV to perform probability calibration with isotonic regression or sigmoid (Platt Scaling).  The parameters within CalibratedClassifierCV that we can adjust are the method ('sigmoid' or 'isotonic') and cv (cross-validation generator).  As we will already be training our models before calibration, we will only use cv = 'prefit'.  Thus, in practice the cross-validation generator will not be a modifiable parameter for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the KNN classifier, we can seek to optimize the following classifier parameters: n-neighbors, weights, and the power parameter ('p')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For KNN the best log loss with hyperparameter tuning is 2.62923629844 with k = 2001 w = uniform p = 1\n",
      "Computation time for this step is 351.40 seconds\n"
     ]
    }
   ],
   "source": [
    "list_for_ks = []\n",
    "list_for_ws = []\n",
    "list_for_ps = []\n",
    "list_for_log_loss = []\n",
    "\n",
    "def k_neighbors_tuned(k,w,p):\n",
    "    tuned_KNN = KNeighborsClassifier(n_neighbors=k, weights=w, p=p).fit(mini_train_data, mini_train_labels)\n",
    "    dev_prediction_probabilities = tuned_KNN.predict_proba(mini_dev_data)\n",
    "    list_for_ks.append(this_k)\n",
    "    list_for_ws.append(this_w)\n",
    "    list_for_ps.append(this_p)\n",
    "    working_log_loss = log_loss(y_true = mini_dev_labels, y_pred = dev_prediction_probabilities, labels = crime_labels_mini_dev)\n",
    "    list_for_log_loss.append(working_log_loss)\n",
    "    #print(\"Multi-class Log Loss with KNN and k,w,p =\", k,\",\",w,\",\", p, \"is:\", working_log_loss)\n",
    "\n",
    "k_value_tuning = [i for i in range(1,5002,500)]\n",
    "weight_tuning = ['uniform', 'distance']\n",
    "power_parameter_tuning = [1,2]\n",
    "\n",
    "start = time.clock()\n",
    "for this_k in k_value_tuning:\n",
    "    for this_w in weight_tuning:\n",
    "        for this_p in power_parameter_tuning:\n",
    "            k_neighbors_tuned(this_k, this_w, this_p)\n",
    "            \n",
    "index_best_logloss = np.argmin(list_for_log_loss)\n",
    "print('For KNN the best log loss with hyperparameter tuning is',list_for_log_loss[index_best_logloss], 'with k =', list_for_ks[index_best_logloss], 'w =', list_for_ws[index_best_logloss], 'p =', list_for_ps[index_best_logloss])\n",
    "end = time.clock()\n",
    "print(\"Computation time for this step is %.2f\" % (end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model calibration:\n",
    "\n",
    "We will consider embeding this step within the for loop for the hyperparameter tuning. More likely we will pipeline it along with the hyperparameter tuning steps.  We will then use GridSearchCV top find the optimized parameters based on our performance metric of Mutli-Class Log Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Log Loss with KNN and k,w,p = 1 , uniform , 1 , sigmoid is: 2.71277990123\n",
      "Multi-class Log Loss with KNN and k,w,p = 1 , uniform , 1 , isotonic is: 2.71416377107\n",
      "Multi-class Log Loss with KNN and k,w,p = 1 , uniform , 2 , sigmoid is: 2.71378604118\n",
      "Multi-class Log Loss with KNN and k,w,p = 1 , uniform , 2 , isotonic is: 2.7168901557\n",
      "Multi-class Log Loss with KNN and k,w,p = 1 , distance , 1 , sigmoid is: 2.71277990123\n",
      "Multi-class Log Loss with KNN and k,w,p = 1 , distance , 1 , isotonic is: 2.71416377107\n",
      "Multi-class Log Loss with KNN and k,w,p = 1 , distance , 2 , sigmoid is: 2.71378604118\n",
      "Multi-class Log Loss with KNN and k,w,p = 1 , distance , 2 , isotonic is: 2.7168901557\n",
      "Multi-class Log Loss with KNN and k,w,p = 501 , uniform , 1 , sigmoid is: 2.60086682706\n",
      "Multi-class Log Loss with KNN and k,w,p = 501 , uniform , 1 , isotonic is: 2.64335037718\n",
      "Multi-class Log Loss with KNN and k,w,p = 501 , uniform , 2 , sigmoid is: 2.60387977714\n",
      "Multi-class Log Loss with KNN and k,w,p = 501 , uniform , 2 , isotonic is: 2.65416869419\n",
      "Multi-class Log Loss with KNN and k,w,p = 501 , distance , 1 , sigmoid is: 2.62772375975\n",
      "Multi-class Log Loss with KNN and k,w,p = 501 , distance , 1 , isotonic is: 2.73181094627\n",
      "Multi-class Log Loss with KNN and k,w,p = 501 , distance , 2 , sigmoid is: 2.62265629187\n",
      "Multi-class Log Loss with KNN and k,w,p = 501 , distance , 2 , isotonic is: 2.73478071804\n",
      "Multi-class Log Loss with KNN and k,w,p = 1001 , uniform , 1 , sigmoid is: 2.59760542997\n",
      "Multi-class Log Loss with KNN and k,w,p = 1001 , uniform , 1 , isotonic is: 2.65541916973\n",
      "Multi-class Log Loss with KNN and k,w,p = 1001 , uniform , 2 , sigmoid is: 2.60664155285\n",
      "Multi-class Log Loss with KNN and k,w,p = 1001 , uniform , 2 , isotonic is: 2.67320210152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bryan/anaconda/lib/python3.6/site-packages/sklearn/calibration.py:507: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. / (1. + np.exp(self.a_ * T + self.b_))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Log Loss with KNN and k,w,p = 1001 , distance , 1 , sigmoid is: 2.62780570553\n",
      "Multi-class Log Loss with KNN and k,w,p = 1001 , distance , 1 , isotonic is: 2.73915999813\n",
      "Multi-class Log Loss with KNN and k,w,p = 1001 , distance , 2 , sigmoid is: 2.6239585973\n",
      "Multi-class Log Loss with KNN and k,w,p = 1001 , distance , 2 , isotonic is: 2.7396359309\n",
      "Multi-class Log Loss with KNN and k,w,p = 1501 , uniform , 1 , sigmoid is: 2.59776199298\n",
      "Multi-class Log Loss with KNN and k,w,p = 1501 , uniform , 1 , isotonic is: 2.68842973364\n",
      "Multi-class Log Loss with KNN and k,w,p = 1501 , uniform , 2 , sigmoid is: 2.6076426784\n",
      "Multi-class Log Loss with KNN and k,w,p = 1501 , uniform , 2 , isotonic is: 2.65509777297\n",
      "Multi-class Log Loss with KNN and k,w,p = 1501 , distance , 1 , sigmoid is: 2.62883969589\n",
      "Multi-class Log Loss with KNN and k,w,p = 1501 , distance , 1 , isotonic is: 2.71513408866\n",
      "Multi-class Log Loss with KNN and k,w,p = 1501 , distance , 2 , sigmoid is: 2.62494342979\n",
      "Multi-class Log Loss with KNN and k,w,p = 1501 , distance , 2 , isotonic is: 2.72607861059\n",
      "Multi-class Log Loss with KNN and k,w,p = 2001 , uniform , 1 , sigmoid is: 2.59748933009\n",
      "Multi-class Log Loss with KNN and k,w,p = 2001 , uniform , 1 , isotonic is: 2.72852555002\n",
      "Multi-class Log Loss with KNN and k,w,p = 2001 , uniform , 2 , sigmoid is: 2.60588224807\n",
      "Multi-class Log Loss with KNN and k,w,p = 2001 , uniform , 2 , isotonic is: 2.67348726558\n",
      "Multi-class Log Loss with KNN and k,w,p = 2001 , distance , 1 , sigmoid is: 2.63057860493\n",
      "Multi-class Log Loss with KNN and k,w,p = 2001 , distance , 1 , isotonic is: 2.73228081995\n",
      "Multi-class Log Loss with KNN and k,w,p = 2001 , distance , 2 , sigmoid is: 2.62594862273\n",
      "Multi-class Log Loss with KNN and k,w,p = 2001 , distance , 2 , isotonic is: 2.73752928972\n",
      "For KNN the best log loss with hyperparameter tuning is 2.59748933009 with k = 2001 w = uniform p = 1 m = sigmoid\n",
      "Computation time for this step is 1994.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here we will calibrate the KNN classifier with both Platt Scaling and with Isotonic Regression using CalibratedClassifierCV\n",
    "# with various parameter settings.  The \"method\" parameter can be set to \"sigmoid\" or to \"isotonic\", \n",
    "# corresponding to Platt Scaling and to Isotonic Regression respectively.\n",
    "\n",
    "list_for_ks = []\n",
    "list_for_ws = []\n",
    "list_for_ps = []\n",
    "list_for_ms = []\n",
    "list_for_log_loss = []\n",
    "\n",
    "def knn_calibrated(k,w,p,m):\n",
    "    tuned_KNN = KNeighborsClassifier(n_neighbors=k, weights=w, p=p).fit(mini_train_data, mini_train_labels)\n",
    "    dev_prediction_probabilities = tuned_KNN.predict_proba(mini_dev_data)\n",
    "    ccv = CalibratedClassifierCV(tuned_KNN, method = m, cv = 'prefit')\n",
    "    ccv.fit(mini_calibrate_data, mini_calibrate_labels)\n",
    "    ccv_prediction_probabilities = ccv.predict_proba(mini_dev_data)\n",
    "    list_for_ks.append(this_k)\n",
    "    list_for_ws.append(this_w)\n",
    "    list_for_ps.append(this_p)\n",
    "    list_for_ms.append(this_m)\n",
    "    working_log_loss = log_loss(y_true = mini_dev_labels, y_pred = ccv_prediction_probabilities, labels = crime_labels_mini_dev)\n",
    "    list_for_log_loss.append(working_log_loss)\n",
    "    print(\"Multi-class Log Loss with KNN and k,w,p =\", k,\",\",w,\",\",p,\",\",m,\"is:\", working_log_loss)\n",
    "\n",
    "k_value_tuning = [i for i in range(1,5002,500)]\n",
    "weight_tuning = ['uniform', 'distance']\n",
    "power_parameter_tuning = [1,2]\n",
    "methods = ['sigmoid', 'isotonic']\n",
    "\n",
    "start = time.clock()\n",
    "for this_k in k_value_tuning:\n",
    "    for this_w in weight_tuning:\n",
    "        for this_p in power_parameter_tuning:\n",
    "            for this_m in methods:\n",
    "                knn_calibrated(this_k, this_w, this_p, this_m)\n",
    "            \n",
    "index_best_logloss = np.argmin(list_for_log_loss)\n",
    "print('For KNN the best log loss with hyperparameter tuning and calibration is',list_for_log_loss[index_best_logloss], 'with k =', list_for_ks[index_best_logloss], 'w =', list_for_ws[index_best_logloss], 'p =', list_for_ps[index_best_logloss], 'm =', list_for_ms[index_best_logloss])\n",
    "end = time.clock()\n",
    "print(\"Computation time for this step is %.2f\" % (end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comments on results for Hyperparameter tuning and Calibration for KNN:\n",
    "\n",
    "We see that the best log loss we achieve for KNN is with _ neighbors, _ weights, and _ power parameter.\n",
    "\n",
    "When we add-in calibration, we see that the the best log loss we achieve for KNN is with _ neighbors, _ weights, _ power parameter, and _ calibration method.\n",
    "\n",
    "(Further explanation here?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial, Bernoulli, and Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Bernoulli Naive Bayes classifier and Multinomial Naive Bayes classifer, we seek to optimize the alpha parameter (Laplace smoothing parameter).  For the Gaussian Naive Bayes classifier there are no inherent parameters within the classifier function to optimize, but we will look at our log loss before and after adding noise to the data that is hypothesized to give it a more normal (Gaussian) distribution, which is required by the GNB classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameter tuning: Bernoulli Naive Bayes\n",
    "\n",
    "For the Bernoulli Naive Bayes classifier, we seek to optimize the alpha parameter (Laplace smoothing parameter) and the binarize parameter (threshold for binarizing of the sample features).  For the binarize parameter, we will create arbitrary thresholds over which our features, which are not binary/boolean features, will be binarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-08 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-07 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-06 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1e-05 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.0001 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.001 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.3 is: 3.61091791264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.01 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.1 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.2 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.3 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.4 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 0.5 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1.0 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 1e-05 is: 3.61091791264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 2.0 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 10.0 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 100.0 , 0.9999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 1e-06 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 1e-05 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.0001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.001 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.01 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.1 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.2 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.3 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.4 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.5 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.6 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.7 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.8 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.9 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.95 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.99 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.999 is: 3.61091791264\n",
      "Multi-class Log Loss with BNB and a,b = 1000.0 , 0.9999 is: 3.61091791264\n",
      "For BNB the best log loss with hyperparameter tuning is 3.61091791264 with alpha = 1e-08 binarization threshold = 1e-06\n",
      "Computation time for this step is 116.42 seconds\n"
     ]
    }
   ],
   "source": [
    "list_for_as = []\n",
    "list_for_bs = []\n",
    "list_for_log_loss = []\n",
    "\n",
    "def BNB_tuned(a,b):\n",
    "    bnb_tuned = BernoulliNB(alpha = a, binarize = b).fit(mini_train_data, mini_train_labels)\n",
    "    dev_prediction_probabilities = bnb_tuned.predict_log_proba(mini_dev_data)\n",
    "    list_for_as.append(this_a)\n",
    "    list_for_bs.append(this_b)\n",
    "    working_log_loss = log_loss(y_true = mini_dev_labels, y_pred = dev_prediction_probabilities, labels = crime_labels_mini_dev)\n",
    "    list_for_log_loss.append(working_log_loss)\n",
    "    print(\"Multi-class Log Loss with BNB and a,b =\", a,\",\",b,\"is:\", working_log_loss)\n",
    "\n",
    "alpha_tuning = [0.00000001,0.0000001,0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 2.0, 10.0, 100.0, 1000.0]\n",
    "binarize_thresholds_tuning = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.999, 0.9999]\n",
    "\n",
    "start = time.clock()\n",
    "for this_a in alpha_tuning:\n",
    "    for this_b in binarize_thresholds_tuning:\n",
    "            BNB_tuned(this_a, this_b)\n",
    "            \n",
    "index_best_logloss = np.argmin(list_for_log_loss)\n",
    "print('For BNB the best log loss with hyperparameter tuning is',list_for_log_loss[index_best_logloss], 'with alpha =', list_for_as[index_best_logloss], 'binarization threshold =', list_for_bs[index_best_logloss])\n",
    "end = time.clock()\n",
    "print(\"Computation time for this step is %.2f\" % (end-start), 'seconds')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameter tuning: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MNB():\n",
    "    mnb = MultinomialNB(alpha = 0.0000001)\n",
    "    mnb.fit(train_data, train_labels)\n",
    "    print(\"\\n\\nMultinomialNB accuracy on dev data:\", mnb.score(dev_data, dev_labels))\n",
    "    \n",
    "    \n",
    "alphas = [0.0, 0.000000001,0.00000001,0.0000001,0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 2.0, 10.0, 100.0, 1000.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tuning: Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GNB():\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train_data, train_labels)\n",
    "    print(\"GaussianNB accuracy on dev data:\", \n",
    "          gnb.score(dev_data, dev_labels))\n",
    "    \n",
    "    # Gaussian Naive Bayes requires the data to have a relative normal distribution. Sometimes\n",
    "    # adding noise can improve performance by making the data more normal:\n",
    "    train_data_noise = np.random.rand(train_data.shape[0],train_data.shape[1])\n",
    "    modified_train_data = np.multiply(train_data,train_data_noise)    \n",
    "    gnb_noise = GaussianNB()\n",
    "    gnb.fit(modified_train_data, train_labels)\n",
    "    print(\"GaussianNB accuracy with added noise:\", \n",
    "          gnb.score(dev_data, dev_labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model calibration:\n",
    "\n",
    "Here we will calibrate the MNB, BNB, and GNB classifiers with both Platt Scaling and with Isotonic Regression using CalibratedClassifierCV with various parameter settings.  The \"method\" parameter can be set to \"sigmoid\" or to \"isotonic\", corresponding to Platt Scaling and to Isotonic Regression respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Logistic Regression classifier, we can seek to optimize the following classifier parameters: penalty (l1 or l2), C (inverse of regularization strength), solver ('newton-cg', 'lbfgs', 'liblinear', or 'sag')\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Decision Tree classifier, we can seek to optimize the following classifier parameters: min_samples_leaf (the minimum number of samples required to be at a leaf node), max_depth\n",
    "\n",
    "From readings, setting min_samples_leaf to approximately 1% of the data points can stop the tree from inappropriately classifying outliers, which can help to improve accuracy (unsure if significantly improves MCLL).\n",
    "\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the SVM classifier, we can seek to optimize the following classifier parameters: C (penalty parameter C of the error term), kernel ('linear', 'poly', 'rbf', sigmoid', or 'precomputed')\n",
    "\n",
    "See source [2] for parameter optimization in SVM\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Neural Networks MLP classifier, we can seek to optimize the following classifier parameters: hidden_layer_sizes, activation ('identity', 'logistic', 'tanh', 'relu'), solver ('lbfgs','sgd', adam'), alpha, learning_rate ('constant', 'invscaling','adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-746a0d71593d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### All the work from Sarah's notebook:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng_mrg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMRG_RandomStreams\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "### All the work from Sarah's notebook:\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "print (theano.config.device) # We're using CPUs (for now)\n",
    "print (theano.config.floatX )# Should be 64 bit for CPUs\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 25\n",
      "Train set = 700000\n",
      "Test set = 78049\n",
      "['DRUG/NARCOTIC', 'RUNAWAY', 'DRUNKENNESS', 'LOITERING', 'STOLEN PROPERTY', 'MISSING PERSON', 'ARSON', 'FRAUD', 'SEX OFFENSES NON FORCIBLE', 'NON-CRIMINAL', 'WEAPON LAWS', 'RECOVERED VEHICLE', 'ASSAULT', 'TRESPASS', 'GAMBLING', 'SUSPICIOUS OCC', 'TREA', 'BAD CHECKS', 'VANDALISM', 'FAMILY OFFENSES', 'DRIVING UNDER THE INFLUENCE', 'WARRANTS', 'PROSTITUTION', 'SEX OFFENSES FORCIBLE', 'DISORDERLY CONDUCT', 'LIQUOR LAWS', 'ROBBERY', 'FORGERY/COUNTERFEITING', 'OTHER OFFENSES', 'EXTORTION', 'VEHICLE THEFT', 'SUICIDE', 'PORNOGRAPHY/OBSCENE MAT', 'LARCENY/THEFT', 'BRIBERY', 'EMBEZZLEMENT', 'SECONDARY CODES', 'KIDNAPPING', 'BURGLARY']\n"
     ]
    }
   ],
   "source": [
    "numFeatures = train_data[1].size\n",
    "numTrainExamples = train_data.shape[0]\n",
    "numTestExamples = test_data.shape[0]\n",
    "print ('Features = %d' %(numFeatures))\n",
    "print ('Train set = %d' %(numTrainExamples))\n",
    "print ('Test set = %d' %(numTestExamples))\n",
    "\n",
    "class_labels = list(set(train_labels))\n",
    "print(class_labels)\n",
    "numClasses = len(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes = 39\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.]] \n",
      "\n",
      "['BURGLARY' 'LARCENY/THEFT' 'OTHER OFFENSES' 'OTHER OFFENSES'\n",
      " 'SUSPICIOUS OCC' 'VANDALISM' 'DRUG/NARCOTIC' 'MISSING PERSON'\n",
      " 'LARCENY/THEFT' 'OTHER OFFENSES'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Binarize the class labels\n",
    "\n",
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size,39))\n",
    "    for j in range(0,data.size):\n",
    "        feature = data[j]\n",
    "        i = class_labels.index(feature)\n",
    "        binarized_data[j,i]=1\n",
    "    return binarized_data\n",
    "\n",
    "train_labels_b = binarizeY(train_labels)\n",
    "test_labels_b = binarizeY(test_labels)\n",
    "numClasses = train_labels_b[1].size\n",
    "\n",
    "print ('Classes = %d' %(numClasses))\n",
    "print ('\\n', train_labels_b[:5, :], '\\n')\n",
    "print (train_labels[:10], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theano' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-8c49129ca7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnumHiddenNodeslayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mw_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumHiddenNodeslayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mw_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumHiddenNodeslayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumHiddenNodeslayer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mw_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumHiddenNodeslayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theano' is not defined"
     ]
    }
   ],
   "source": [
    "###1) Parameters\n",
    "numFeatures = train_data.shape[1]\n",
    "\n",
    "numHiddenNodeslayer1 = 50\n",
    "numHiddenNodeslayer2 = 30\n",
    "\n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodeslayer1))*0.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodeslayer1, numHiddenNodeslayer2))*0.01)))\n",
    "w_3 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodeslayer2, numClasses))*0.01)))\n",
    "params = [w_1, w_2, w_3]\n",
    "\n",
    "\n",
    "###2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "srng = RandomStreams()\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        X *= srng.binomial(X.shape, p=1 - p)\n",
    "        X /= 1 - p\n",
    "    return X\n",
    "\n",
    "def model(X, w_1, w_2, w_3, p_1, p_2, p_3):\n",
    "    return T.nnet.softmax(T.dot(dropout(T.nnet.sigmoid(T.dot(dropout(T.nnet.sigmoid(T.dot(dropout(X, p_1), w_1)),p_2), w_2)),p_3),w_3))\n",
    "y_hat_train = model(X, w_1, w_2, w_3, 0.2, 0.5,0.5)\n",
    "y_hat_predict = model(X, w_1, w_2, w_3, 0., 0., 0.)\n",
    "\n",
    "### (3) Cost function\n",
    "cost = T.mean(T.sqr(y_hat - Y))\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat_train, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-c7f0e6495c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cost' is not defined"
     ]
    }
   ],
   "source": [
    "### (4) Objective (and solver)\n",
    "\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for wi, grad in zip(w, grads):\n",
    "        updates.append([wi, wi - grad * alpha])\n",
    "    return updates\n",
    "\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat_predict, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 10 \n",
    "\n",
    "def gradientDescent(epochs):\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cc = train(train_data[start:end], train_labels_b[start:end])\n",
    "        clear_output(wait=True)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))) )\n",
    "\n",
    "gradientDescent(50)\n",
    "\n",
    "### How to decide what # to use for epochs? epochs in this case are how many rounds?\n",
    "### plot costs for each of the 50 iterations and see how much it decline.. if its still very decreasing, you should\n",
    "### do more iterations; otherwise if its looking like its flattening, you can stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Random Forest classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_features, max_depth, min_samples_leaf, bootstrap (whether or not bootstrap samples are used when building trees), oob_score (whether or not out-of-bag samples are used to estimate the generalization accuracy)\n",
    "\n",
    "###### Model calibration:\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "There are no major changes that we seek to make in the AdaBoostClassifier with respect to default parameter values.\n",
    "\n",
    "###### Adaboosting each classifier:\n",
    "\n",
    "We will run the AdaBoostClassifier on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Bagging meta classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_samples, max_features, bootstrap (whether or not bootstrap samples are used when building trees), bootstrap_features (whether features are drawn with replacement), and oob_score (whether or not out-of-bag samples are used to estimate the generalization accuracy)\n",
    "\n",
    "###### Bagging each classifier:\n",
    "\n",
    "We will run the BaggingClassifier on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier\n",
    "\n",
    "###### Hyperparameter tuning:\n",
    "\n",
    "For the Gradient Boosting meta classifier, we can seek to optimize the following classifier parameters: n_estimators (the number of trees in the forsest), max_depth, min_samples_leaf, and max_features\n",
    "\n",
    "###### Gradient Boosting each classifier:\n",
    "\n",
    "We will run the GradientBoostingClassifier with loss = 'deviance' (as loss = 'exponential' uses the AdaBoost algorithm) on each different classifier from above, using the classifier settings with optimized Multi-class Log Loss after hyperparameter tuning and calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we will likely use Pipeline and GridSearchCV in order to find the overall classifier with optimized Multi-class Log Loss.\n",
    "# This will be the last step after all attempts at feature addition, hyperparameter tuning, and calibration are completed\n",
    "# and the corresponding performance metrics are gathered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1) Hsiang, Solomon M. and Burke, Marshall and Miguel, Edward. \"Quantifying the Influence of Climate on Human Conflict\". Science, Vol 341, Issue 6151, 2013   \n",
    "\n",
    "2) Huang, Cheng-Lung. Wang, Chieh-Jen. \"A GA-based feature selection and parameters optimization for support vector machines\". Expert Systems with Applications, Vol 31, 2006, p 231-240\n",
    "\n",
    "3) More to come \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
