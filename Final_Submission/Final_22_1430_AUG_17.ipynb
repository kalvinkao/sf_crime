{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle San Francisco Crime Classification\n",
    "## Berkeley MIDS W207 Final Project\n",
    "### Sarah Cha, Sam Goodgame, Kalvin Kao, Bryan Moore\n",
    "\n",
    "**This project ingests data about crimes committed in San Francisco and predicts their types. It is part of a [Kaggle competition](https://www.kaggle.com/c/sf-crime).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task in this project is to predict the type of a crime based on its component details. The component details are primarily related to time and location.\n",
    "\n",
    "This is an interesting problem space because time and location are both high-dimensional variables. Such variables don't tend to work well with machine learning models, because they lead the models to overfit and generalize poorly. \n",
    "\n",
    "Accordingly, our goal in this project is to generate an accurate, parsimonious model by working our way through the model selection triple:\n",
    "- Model selection\n",
    "- Feature Engineering\n",
    "- Hyperparameter tuning (and calibration)\n",
    "\n",
    "While the model selection triple may seem like a linear checklist, we did not approach it that way. In other words, we conducted model selection, feature engineering, and hyperparameter tuning in parallel, with different members of the team focusing on different aspects of the problem at the same time. \n",
    "\n",
    "After transforming the data into a usable format, we set about engineering useful features. We focused primarily on enriching our data with features related to weather and schools.\n",
    "\n",
    "Then, we protoyped the major model types using their default specifications. Then, we dug slightly deeper into each model type by tuning the hyperparameters and calibrating the models. \n",
    "\n",
    "#### Note\n",
    "This notebook only includes code relevant to the winning model. However, we tested and optimized many models (perhaps even tens of thousands of model specifications) in order to converge on our final model. To view our entire approach to this problem, please refer to the appropriate Jupyter notebook in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling libraries and estimators\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Meta-estimators\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Calibration tools\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Set random seed and format print output\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step was to transform the data into a form that we could use for machine learning: a Numpy array with a single column of targets (in this case, the type of crime committed) and many columns of independent/predictor variables.\n",
    "\n",
    "There are many ways to wrangle the data; we chose SQL for the first step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use DDL below to construct a table for the data within a PostgreSQL database:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE kaggle_sf_crime (\n",
    "dates TIMESTAMP,                                \n",
    "category VARCHAR,\n",
    "descript VARCHAR,\n",
    "dayofweek VARCHAR,\n",
    "pd_district VARCHAR,\n",
    "resolution VARCHAR,\n",
    "addr VARCHAR,\n",
    "X FLOAT,\n",
    "Y FLOAT);\n",
    "```\n",
    "#### Move the training data, downloaded from Kaggle as a .csv file, into the PostgreSQL database:\n",
    "```sql\n",
    "\\copy kaggle_sf_crime FROM '/Users/Goodgame/Desktop/MIDS/207/final/sf_crime_train.csv' DELIMITER ',' CSV HEADER;\n",
    "```\n",
    "\n",
    "#### Use SQL to transform the data into a usable form:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  category,\n",
    "  date_part('hour', dates) AS hour_of_day,\n",
    "  CASE\n",
    "    WHEN dayofweek = 'Monday' then 1\n",
    "    WHEN dayofweek = 'Tuesday' THEN 2\n",
    "    WHEN dayofweek = 'Wednesday' THEN 3\n",
    "    WHEN dayofweek = 'Thursday' THEN 4\n",
    "    WHEN dayofweek = 'Friday' THEN 5\n",
    "    WHEN dayofweek = 'Saturday' THEN 6\n",
    "    WHEN dayofweek = 'Sunday' THEN 7\n",
    "  END AS dayofweek_numeric,\n",
    "  X,\n",
    "  Y,\n",
    "  CASE\n",
    "    WHEN pd_district = 'BAYVIEW' THEN 1\n",
    "    ELSE 0\n",
    "  END AS bayview_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'INGLESIDE' THEN 1\n",
    "    ELSE 0\n",
    "  END AS ingleside_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'NORTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS northern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'CENTRAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS central_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'MISSION' THEN 1\n",
    "    ELSE 0\n",
    "  END AS mission_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'SOUTHERN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS southern_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TENDERLOIN' THEN 1\n",
    "    ELSE 0\n",
    "  END AS tenderloin_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'PARK' THEN 1\n",
    "    ELSE 0\n",
    "  END AS park_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'RICHMOND' THEN 1\n",
    "    ELSE 0\n",
    "  END AS richmond_binary,\n",
    "    CASE\n",
    "    WHEN pd_district = 'TARAVAL' THEN 1\n",
    "    ELSE 0\n",
    "  END AS taraval_binary\n",
    "FROM kaggle_sf_crime;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "#### Location One-Hot Encoding\n",
    "Location information tends to be extremely high-dimensional, which presents difficulties for modeling. The original dataset's location information came in three major formats: \n",
    "1. X and Y coordinates\n",
    "2. Street address information\n",
    "3. Police department districts\n",
    "\n",
    "After visualizing the data and conducting basic exploratory data analysis, we decided to use one-hot encoding the transform the police department location information into features. In other words, each police department becomes a feature, and a given observation receives a value of '1' if it occured in the police department described by the feature. Otherwise, it received a value of '0'. Approaching location in this way allowed us to preserve the parsimony of our model; it retains the majority of the important variance in the data without overfitting. \n",
    "\n",
    "#### Time One-Hot Encoding\n",
    "We took the same approach to simplifying time data. Time is also high-dimensional; if we were to try to model crimes based on the provided timestamps (which include seconds), then the eight weeks of data generate 4,838,400 possibilities. We used one-hot encoding to simplify those possibilities into one 7-dimensional day-of-week feature, and an addition 24-dimensional hour-of-day feature. We beleive that these features capture the most variation from the data and allow our models to generalize without risking overfitting.\n",
    "\n",
    "### Weather Data\n",
    "We sought to add features to our models that improved performance with respect to out desired performance metric.  Scientists before us have documented correlations between weather patterns and crime; some experts even argue for a causal relationship between weather and crime [1].  More specifically, a 2013 paper published in *Science* demonstrated that higher temperatures and extreme rainfall led to large increases in conflict.  Due to this research, we see weather as a source for additional features to improve the performance of our classifiers.  We gathered weather data from the National Centers for Environmental Information --specifically, teh National Oceanic and Atmospheric Administration.\n",
    "\n",
    "We selected the following weather features, which we felt captured the overall state of the weather at the time of each example, and which also had enough datapoints to be useful: \n",
    "- 'HOURLYDRYBULBTEMPF'-- hourly ‘dry bulb’ temperature (*F)\n",
    "- 'HOURLYRelativeHumidity'-- hourly ‘relative’ humidity (%)\n",
    "- 'HOURLYWindSpeed'-- hourly wind speed (mph)\n",
    "- 'HOURLYSeaLevelPressure'-- hourly sea level pressure (in-Hg)\n",
    "- 'HOURLYVISIBILITY'-- hourly visibility (miles).\n",
    "\n",
    "In addition, we created a variable, ‘Daylight’, a daylight indicator (1 if time of sunrise < timestamp < time of sunset, and 0 otherwise).  We engineered this feature by comparing the timestamp of each crime example to the time of sunrise (‘DAILYSunrise’) and the time of sunset (‘DAILYSunset’) for the date of the example.\n",
    "\n",
    "Extracting and creating these additional features was tricky since there were differences in formatting between the timestamp of each crime example, the timestamp of each weather timepoint, and the times listed for ‘DAILYSunrise’ and ‘DAILYSunset’.\n",
    "\n",
    "Additionally, we investigated usage of sky conditions (‘HOURLYSKYCONDITIONS’), weather type (‘HOURLYPRSENTWEATHERTYPE’), and total precipitation over the past hour (‘HOURLYPrecip’), but these variables resulted in missing data for some examples, and also additional processing that we did not feel would substantially increase the amount of information captured by the other weather variables we selected.  For example, the ‘HOURLYPRSENTWEATHERTYPE’ was an initially attractive feature since it explicitly states such weather conditions as fog and rain, but it is a categorical variable that has over 100 possible values.  The data for this variable was not available for many crime examples, and some weather timepoints contain multiple conflicting values in this variable (multiple observers).  Furthermore, binarization of this variable would have increased the dimensionality of our data by an order of magnitude.\n",
    "\n",
    "### School Data\n",
    "\n",
    "We also found government data from data.gov that links each zip code in the country to longitude and latitude coordinates, in an effort to reduce location dimensions from 2 to 1. We used a custom distance function to map each set of longitude, latitude coordinates associated with each crime incidents to find the close 5-digit zip code using the data.gov file. Unfortunately even after we narrowed the data California zip codes (> 94000 and < 95000) we were unable to run the data in time to add the feature to the data set.\n",
    "\n",
    "In addition, we found a data set offered by the California Department of Education (cde.ca.gov) with a list of all schools (K-12) in California and accompanying characteristics of those schools including active status, grades taught (elementary, middle vs. high school), and address including location coordinates. We created a distance function to match the longitude, latitude coordinates of each of crime to the closest school and pull other potentially interesting features including: ‘closest_school’ - name of closest school, ‘school_distance’ - euclidean distance between lat/long coordinates of crime and lat/long coordinates of school, and ‘school_type’ - whether it’s an elementary school, high school, etc.\n",
    "\n",
    "Ultimately, the technical challenges in incorporating these features amounted in an unexpectedly high computational requirement that we weren’t able to meet in time for this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "Once we generated our features, we tranformed them into the appropriate formats. We also broke the data into subsets for training and development. We also created smaller subsets for prototyping quickly.\n",
    "\n",
    "One point of note: we removed two categories from the data. The crimes labelled 'TREA' (treason) and 'PORNOGRAPHY/OBSCENE MAT' occurred so infrequently that they presented difficulties for modeling. The best approach was simply to remove them. We assume the risk of incorrectly identifying these crimes in the test set, but doing so allows us to proceed with modeling unhindered by off-by-one errors.\n",
    "\n",
    "The final CSV we use for modeling is [here](https://drive.google.com/file/d/0B74-LZykH7Cud0hmYXBjNzZCaEU/view?ts=5998bc78)--to follow along with our code, download it and adjust the first line of the code below to reference wherever it's stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There should be 37 of each type of label -- otherwise, we'll have an issue later:\n",
      "37 37 37 37\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/Goodgame/Desktop/prototyping/x_data_3.csv\"\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "x_data = df.drop('category', 1)\n",
    "y = df.category.as_matrix()\n",
    "\n",
    "# Impute missing values with mean values:\n",
    "x_complete = x_data.fillna(x_data.mean())\n",
    "X_raw = x_complete.as_matrix()\n",
    "\n",
    "# Scale the data between 0 and 1:\n",
    "X = MinMaxScaler().fit_transform(X_raw)\n",
    "\n",
    "# Shuffle data to remove any underlying pattern that may exist\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "'''Due to difficulties with log loss and set(y_pred) needing to match set(labels), \n",
    "we will remove the extremely rare crimes from the data for quality issues.'''\n",
    "X_minus_trea = X[np.where(y != 'TREA')]\n",
    "y_minus_trea = y[np.where(y != 'TREA')]\n",
    "X_final = X_minus_trea[np.where(y_minus_trea != 'PORNOGRAPHY/OBSCENE MAT')]\n",
    "y_final = y_minus_trea[np.where(y_minus_trea != 'PORNOGRAPHY/OBSCENE MAT')]\n",
    "\n",
    "# Separate training, dev, and test data:\n",
    "all_train_data, all_train_labels = X, y\n",
    "dev_data, dev_labels = X_final[700000:], y_final[700000:]\n",
    "train_data, train_labels = X_final[100000:700000], y_final[100000:700000]\n",
    "calibrate_data, calibrate_labels = X_final[:100000], y_final[:100000]\n",
    "\n",
    "# Mini datasets for quick prototyping\n",
    "mini_train_data, mini_train_labels = X_final[:20000], y_final[:20000]\n",
    "mini_calibrate_data, mini_calibrate_labels = X_final[19000:28000], y_final[19000:28000]\n",
    "mini_dev_data, mini_dev_labels = X_final[49000:60000], y_final[49000:60000]\n",
    "\n",
    "# Create list of the crime type labels.  \n",
    "# This will act as the \"labels\" parameter for the log loss functions that follow\n",
    "\n",
    "crime_labels = list(set(y_final))\n",
    "crime_labels_mini_train = list(set(mini_train_labels))\n",
    "crime_labels_mini_dev = list(set(mini_dev_labels))\n",
    "crime_labels_mini_calibrate = list(set(mini_calibrate_labels))\n",
    "\n",
    "print(\"There should be 37 of each type of label -- otherwise, we'll have an issue later:\")\n",
    "print(len(crime_labels), len(crime_labels_mini_train), len(crime_labels_mini_dev),len(crime_labels_mini_calibrate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Formatting Test Data\n",
    "#### The test data needs the same transformations we applied to the training data.\n",
    "\n",
    "*Download the transformed test data from [here](https://drive.google.com/a/berkeley.edu/uc?id=0B74-LZykH7CuYjF5dzN6aHl0YVU&export=download).*\n",
    "\n",
    "*Download the sample submission from [here](https://www.kaggle.com/c/sf-crime/download/sampleSubmission.csv.zip).*\n",
    "\n",
    "To follow along with the notebook, ensure the paths to both in the code below are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Kaggle submission format requires listing the ID of each example.\n",
    "# This is to remember the order of the IDs after shuffling\n",
    "allIDs = np.array(list(df.axes[0]))\n",
    "allIDs = allIDs[shuffle]\n",
    "\n",
    "testIDs = allIDs[800000:]\n",
    "devIDs = allIDs[700000:800000]\n",
    "trainIDs = allIDs[:700000]\n",
    "\n",
    "# Extract the column names for the required submission format\n",
    "sampleSubmission_path = \"/Users/Goodgame/Desktop/prototyping/sampleSubmission.csv\"\n",
    "sampleDF = pd.read_csv(sampleSubmission_path)\n",
    "allColumns = list(sampleDF.columns)\n",
    "featureColumns = allColumns[1:]\n",
    "\n",
    "# Extracting the test data for a baseline submission\n",
    "real_test_path = \"/Users/Goodgame/Desktop/prototyping/test_data_transformed.csv\"\n",
    "testDF = pd.read_csv(real_test_path, header=0)\n",
    "real_test_data = testDF\n",
    "\n",
    "test_complete = real_test_data.fillna(real_test_data.mean())\n",
    "Test_raw = test_complete.as_matrix()\n",
    "\n",
    "test_data = MinMaxScaler().fit_transform(Test_raw)\n",
    "\n",
    "# Remember the ID of each test data point, in case we shuffle the test data\n",
    "testIDs = list(testDF.axes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the code above will shuffle data differently every time it's run, so model accuracies will vary accordingly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Performance Criteria\n",
    "\n",
    "As determined by the Kaggle submission guidelines, the performance criteria metric for the San Francisco Crime Classification competition is Multi-class Logarithmic Loss (also known as cross-entropy).  There are various other performance metrics that are appropriate for different domains: accuracy, F-score, Lift, ROC Area, average precision, precision/recall break-even point, and squared error.\n",
    "\n",
    "Because the Kaggle competition guidelines use multi-class logarithmic loss to score submissions, we will use that metric to gauge the effectiveness of our models in development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Calibration\n",
    "\n",
    "During the course of this project, we tested dozens of models and tens of thousands of model specifications:\n",
    "\n",
    "##### 1) Hyperparameter tuning\n",
    "\n",
    "Each classifier has parameters that we can engineer to further optimize performance, as opposed to using the default parameter values. The approach is specific to each model type.\n",
    "\n",
    "##### 2) Model calibration\n",
    "\n",
    "After tuning hyperparameters, we calibrated the models via Platt Scaling or Isotonic Regression to attempt to improve their performance.\n",
    "\n",
    "We used CalibratedClassifierCV to perform probability calibration with isotonic regression or sigmoid (Platt Scaling).  The parameters within CalibratedClassifierCV allowed us to adjust the method ('sigmoid' or 'isotonic') and cv (cross-validation generator). Because we train our models before calibration, we only use cv = 'prefit'.  Therefore, in practice, the cross-validation generator is not a modifiable parameter for our pipeline.\n",
    "\n",
    "### Models we tuned and calibrated\n",
    "#### To see more about our work with these models, please reference the additional Jupyter notebooks in this repository.\n",
    "1. Multinomial Naive Bayes\n",
    "2. Bernoulli Naive Bayes\n",
    "3. Gaussian Naive Bayes\n",
    "4. Logistic Regression\n",
    "5. Neural Networks (from Theano and MLPClassifier)\n",
    "6. Decision Trees\n",
    "7. K-Nearest Neighbors\n",
    "\n",
    "**Additionally, we examined three types of meta-estimators:**\n",
    "\n",
    "1. AdaBoost Classifier\n",
    "2. Bagging Classifier\n",
    "3. Gradient Boosting Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Winning Model: Random Forest\n",
    "\n",
    "For the Random Forest classifier, we optimized the following classifier parameters: \n",
    "1. n_estimators (the number of trees in the forsest) \n",
    "2. max_features\n",
    "3. max_depth\n",
    "4. min_samples_leaf\n",
    "5. bootstrap (whether or not bootstrap samples are used when building trees)\n",
    "6. oob_score (whether or not out-of-bag samples are used to estimate the generalization accuracy)\n",
    "\n",
    "### Parallelizing GridSearchCV with Spark-sklearn \n",
    "\n",
    "To optimize the parameters, we used GridSearchCV -- with a slight wrinkle. Because we needed GridSearchCV to sort through an incredible number of model specifications with a very large amount of data, we decided to parallelize the process using Spark.\n",
    "\n",
    "Fortunately, there is a PyPI library for doing just that: **spark-sklearn**. Check out the package [here](http://pythonhosted.org/spark-sklearn/).\n",
    "\n",
    "In order to run spark-sklearn, we took the following steps: \n",
    "- Create an AWS EC2 instance (in our case, a c3.8xlarge instance with an Ubuntu Linux operating system, with 32 vCPUs and 60GiB of memory)\n",
    "- Install: Java, Scala, Anaconda, pip, and relevant dependencies (key library: spark_sklearn)\n",
    "- Run GridSearchCV within a SparkContext\n",
    "\n",
    "All of the code is the exact same as a normal GridSearchCV with scikit-learn, except for two lines:\n",
    "\n",
    "$ *from spark_sklearn import GridSearchCV*\n",
    "\n",
    "$ *gs = GridSearchCV(**sc**, clf, param_grid)*\n",
    "\n",
    "\n",
    "In other words, the grid search takes SparkContext as an extra parameter. Because of that, the process can be parallelized across multiple cores, which saves a lot of time.\n",
    "\n",
    "For more information on parallelizing GridSearchCV using Spark, see this DataBricks [tutorial](https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-apache-spark.html) and this [AWS EC2 PySpark tutorial](https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297). *Note: we ran the PySpark code in the PySpark REPL, rather than in a script. We hit issues with dependencies using Python scripts. We appear not to be alone in this issue; [other data scientists](https://twitter.com/sarah_guido/status/672880303891947520?lang=en) have also hit a wall using scikit-learn with Spark.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Accuracy After Hyperparameter Tuning: 2.36593209104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation on test data\n",
    "\n",
    "During development, we were able to achieve a multi-class logarithmic loss of 2.36593209104 using the model specification and calibration below. The isotonic calibration yielded slightly more accuracy than the sigmoid calibration.\n",
    "\n",
    "The code in the cell below will generate a .csv file with the predictions on test data. It can take about 25 minutes to run, depending on your environment and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_predictions():\n",
    "\n",
    "    random_forest_tuned = RandomForestClassifier(min_impurity_split=1, \n",
    "                                       n_estimators=100, \n",
    "                                       bootstrap= True,\n",
    "                                       max_features=15,\n",
    "                                       criterion='entropy',\n",
    "                                       min_samples_leaf=10,\n",
    "                                       max_depth=None\n",
    "                                      ).fit(X_final, y_final)\n",
    "    rf_isotonic = CalibratedClassifierCV(random_forest_tuned, method = 'isotonic', cv = 'prefit')\n",
    "    rf_isotonic.fit(all_train_data, all_train_labels)\n",
    "    return rf_isotonic.predict_proba(test_data)\n",
    "\n",
    "predictions = generate_predictions()\n",
    "resultDF = pd.DataFrame(predictions, columns=featureColumns)\n",
    "\n",
    "# Add the IDs as a final column\n",
    "resultDF.loc[:,'Id'] = pd.Series(testIDs,index=resultDF.index)\n",
    "\n",
    "# Make the 'Id' column the first column, per the requirements\n",
    "colnames = resultDF.columns.tolist()\n",
    "colnames = colnames[-1:] + colnames[:-1]\n",
    "resultDF = resultDF[colnames]\n",
    "\n",
    "# Output to a .csv file\n",
    "resultDF.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Data Log Loss\n",
    "\n",
    "Run this code to view our log loss on development data.\n",
    "\n",
    "Warning: it takes approximately 25 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_DT_calibrate_isotonic = RandomForestClassifier(min_impurity_split=1, \n",
    "                                       n_estimators=100, \n",
    "                                       bootstrap= True,\n",
    "                                       max_features=15,\n",
    "                                       criterion='entropy',\n",
    "                                       min_samples_leaf=10,\n",
    "                                       max_depth=None\n",
    "                                      ).fit(train_data, train_labels)\n",
    "\n",
    "ccv_isotonic = CalibratedClassifierCV(tuned_DT_calibrate_isotonic, method = 'isotonic', cv = 'prefit')\n",
    "ccv_isotonic.fit(calibrate_data, calibrate_labels)\n",
    "ccv_prediction_probabilities_isotonic = ccv_isotonic.predict_proba(dev_data)\n",
    "working_log_loss_isotonic = log_loss(y_true = dev_labels, \n",
    "                                     y_pred = ccv_prediction_probabilities_isotonic, \n",
    "                                     labels = crime_labels)\n",
    "\n",
    "print(\"Multi-class Log Loss with Random Forest and isotonic calibration:\", working_log_loss_isotonic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "To keep the code in this notebook streamlined towards its goal -- predicting the types of crimes in test data -- we've conducted the error analysis in separate notebooks in this repository. Please view the error analysis notebook separately.\n",
    "\n",
    "Having said that, you can view our confusion matrix in its entirety [here](https://drive.google.com/file/d/0B74-LZykH7CuV0pxZ3VjcktnbWc/view?ts=599bae16).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1) Hsiang, Solomon M. and Burke, Marshall and Miguel, Edward. \"Quantifying the Influence of Climate on Human Conflict\". Science, Vol 341, Issue 6151, 2013   \n",
    "\n",
    "2) Huang, Cheng-Lung. Wang, Chieh-Jen. \"A GA-based feature selection and parameters optimization for support vector machines\". Expert Systems with Applications, Vol 31, 2006, p 231-240\n",
    "\n",
    "3) https://gallery.cortanaintelligence.com/Experiment/Evaluating-and-Parameter-Tuning-a-Decision-Tree-Model-1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
